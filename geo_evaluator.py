# -*- coding: utf-8 -*-
# geo_evaluator.py

import json
import os
import re
import time
import hashlib  # â­ æ–°å¢
import math
from statistics import mean, pstdev
from typing import Dict, Literal, TypedDict, Optional, Any
from openai import OpenAI

from geo_metrics import compression_ratio, type_token_ratio, reading_ease
from geo_report import render_report_html
from geo_seal import seal_metrics   # é¡¶éƒ¨åŠ è¿™ä¸€è¡Œ

from providers_groq_gemini import ModelHub
hub = ModelHub()

# ========= ç»Ÿä¸€ LLM è°ƒç”¨ï¼ˆæ”¯æŒæ‰‹åŠ¨ + auto fallbackï¼‰ =========
from pipeline.inference_engine import call_model

# ========= ä½ éœ€è¦æŠŠè¿™é‡Œæ¥åˆ°ä½ å·²æœ‰çš„ DashScope/DeepSeek è°ƒç”¨ =========
def llm_complete(
    model_name: str,
    prompt: str,
    temperature: float = 0.0,
    max_tokens: int = 32,
    provider: str = "groq",
) -> str:
    """
    è¯„åˆ†ä¸“ç”¨è°ƒç”¨ï¼ˆä¸¥æ ¼çŸ­è¾“å‡ºï¼‰ï¼š
    - provider å¯é€‰ groq / gemini / grok / deepseek / qwen
    - model_name ä½œä¸ºå…·ä½“æ¨¡å‹åé€ä¼ 
    """
    # è¯„åˆ†æç¤ºï¼šåªè¦æ•°å­—
    sys_hint = "You are a strict grader. Output ONE number only."
    full_prompt = f"{sys_hint}\n\n{prompt}"

    # ç»Ÿä¸€èµ° call_model
    return (call_model(
        full_prompt,
        provider=provider,
        temperature=temperature,
        model=model_name,
    ) or "").strip()

# ç®€å•çš„è¿›ç¨‹å†…ç¼“å­˜ï¼šåŒä¸€æ¨¡å‹ + åŒä¸€é—®å¥ + åŒä¸€åŸæ–‡ + åŒä¸€æ”¹å†™ â†’ åªè¯„ä¸€æ¬¡
_GEO_BASE_CACHE: Dict[str, Dict[str, Any]] = {}

# =========================
# GEO Cache Debug Logging
# =========================
GEO_CACHE_DEBUG = os.getenv("GEO_CACHE_DEBUG", "0").strip() == "1"

def _cache_dbg(tag: str, hit: bool, cache_key: str, extra: dict | None = None):
    """
    ä»…åœ¨ GEO_CACHE_DEBUG=1 æ—¶æ‰“å° cache hit/miss
    """
    if not GEO_CACHE_DEBUG:
        return
    try:
        ck = (cache_key or "")[:24] + "..." if cache_key else ""
        payload = {"tag": tag, "hit": bool(hit), "key": ck}
        if extra:
            payload.update(extra)
        print("[GEO-CACHE]", json.dumps(payload, ensure_ascii=False))
    except Exception:
        # debug ä¸å½±å“ä¸»æµç¨‹
        print(f"[GEO-CACHE] tag={tag} hit={hit}")



def _make_cache_key(
    model_ui: str,
    model_name: str,
    user_question: str,
    article_title: str,
    source_text: str,
    rewritten_text: str,
) -> str:
    """
    ä¸ºå•æ¬¡ GEO è¯„ä¼°ç”Ÿæˆç¨³å®š keyï¼š
    - åŒä¸€æ¨¡å‹ + åŒä¸€ provider + åŒä¸€é—®å¥ + åŒä¸€æ ‡é¢˜ + åŒä¸€åŸæ–‡ + åŒä¸€æ”¹å†™ â†’ key ç›¸åŒ
    """
    payload = json.dumps(
        {
            "ui": model_ui,
            "model": model_name,
            "q": user_question,
            "title": article_title,
            "src": source_text,
            "opt": rewritten_text,
        },
        ensure_ascii=False,
        sort_keys=True,
    )
    return hashlib.sha256(payload.encode("utf-8")).hexdigest()



class GeoScore(TypedDict):
    relevance: float
    influence: float
    uniqueness: float
    diversity: float
    subjective_position: float
    subjective_count: float
    follow_up: float
    objective: Dict[str, float]
    geo_score: float
    mode: Literal["single_text", "with_citations"]
    model_used: str
    latency_ms: int
    samples: int
    stddev: Dict[str, float]

# æ”¯æŒ 1â€“20ï¼ˆå¯å¸¦å°æ•°ï¼‰ï¼Œå¹¶é¿å…è¯¯æŠ“åˆ°æ›´å¤§æ•°å­—çš„å°¾å·´
_NUM_RE = re.compile(r'(?<!\d)(?:20(?:\.\d+)?|1?\d(?:\.\d+)?)(?!\d)')


def _clip_1_5(x: float) -> float:
    return max(1.0, min(5.0, x))

def _clip_1_20(x: float) -> float:
    return max(1.0, min(20.0, float(x)))


def _extract_score(text: str) -> Optional[float]:
    """ä» LLM è¿”å›ä¸­æŠ“å– 1~5 çš„æ•°å­—ï¼›å–æœ€åä¸€ä¸ªåŒ¹é…ä»¥é˜²å‰é¢æ˜¯ç¤ºä¾‹ã€‚"""
    matches = _NUM_RE.findall(text)
    if not matches:
        return None
    try:
        return float(matches[-1])
    except ValueError:
        return None

def load_eval_templates(dir_path: str = "templates/") -> Dict[str, str]:
    def _read(name: str) -> str:
        p = os.path.join(dir_path, name)
        with open(p, "r", encoding="utf-8") as f:
            return f.read()
    return {
        "relevance": _read("relevance_detailed.txt"),
        "influence": _read("influence_detailed.txt"),
        "uniqueness": _read("uniqueness_detailed.txt"),
        "diversity": _read("diversity_detailed.txt"),
        "subjpos": _read("subjpos_detailed.txt"),
        "subjcount": _read("subjcount_detailed.txt"),
        "follow": _read("follow_detailed.txt"),
    }

def _format_prompt(tpl: str, query: str, answer: str) -> str:
    """
    å°†æ¨¡æ¿ä¸­ç¤ºä¾‹æ®µè½æ›¿æ¢ä¸ºå®é™…çš„ {query}/{answer}ã€‚
    æ¨¡æ¿è‹¥å«å ä½ç¬¦ {query} / {answer} æœ€ä½³ï¼›è‹¥æ— å ä½ï¼Œä¹Ÿç›´æ¥é™„åŠ åœ¨æœ«å°¾ã€‚
    """
    has_q = "{query}" in tpl
    has_a = "{answer}" in tpl
    if has_q or has_a:
        return tpl.replace("{query}", query).replace("{answer}", answer)
    return f"{tpl.strip()}\n\nInput User Query:\n{query}\n\nGenerated Answer:\n{answer}\n\nEvaluation Form (scores ONLY):\n- Score:"

FORCE_NUMERIC_SUFFIX = """
You MUST output exactly one Arabic number between 1 and 20.
You MAY use decimals like 16.5.
Do NOT output any words or explanations, only the number.

ä¾‹å¦‚ï¼šåªè¾“å‡ºä¸€ä¸ª 1 åˆ° 20 ä¹‹é—´çš„é˜¿æ‹‰ä¼¯æ•°å­—ï¼Œå¯ä»¥å¸¦å°æ•°ï¼Œå¦‚ï¼š16.5
"""



def evaluate_dimension(
    model_name: str,
    prompt_template: str,
    query: str,
    answer: str,
    provider: str = "groq",
    mode: Literal["single_text", "with_citations"] = "single_text",
    retries: int = 2
) -> float:
    """
    è¯„åˆ†ä¸¥æ ¼æ¨¡å¼ï¼šåœ¨æ¨¡æ¿åè¿½åŠ â€œåªè¾“å‡ºæ•°å­—â€æç¤ºï¼›è§£æå¤±è´¥è‡ªåŠ¨é‡è¯•ï¼Œæœ€ç»ˆå›é€€12.0ã€‚
    """
    base_prompt = _format_prompt(prompt_template, query, answer)
    prompt = f"{base_prompt.strip()}\n\n{FORCE_NUMERIC_SUFFIX.strip()}"
    last_err = None
    for _ in range(max(1, retries)):
        try:
            text = llm_complete(model_name, prompt, provider=provider, temperature=0.0, max_tokens=12)
            val = _extract_score(text or "")
            if val is not None:
                return _clip_1_20(val)

        except Exception as e:
            last_err = e
            time.sleep(0.2)
    return 12.0

def _sample_scores(
    model_name: str,
    tpl: str,
    query: str,
    answer: str,
    n: int,
    provider: str = "auto"
) -> (float, float):
    vals = [
        evaluate_dimension(model_name, tpl, query, answer, provider=provider)
        for _ in range(max(1, n))
    ]
    return float(mean(vals)), float(pstdev(vals)) if len(vals) > 1 else 0.0

def evaluate_subjective_scores(
    model_name: str,
    query: str,
    answer: str,
    provider: str = "groq",
    samples: int = 1
) -> (Dict[str, float], Dict[str, float]):
    """
    è¿”å›ï¼šä¸ƒç»´å¹³å‡åˆ†ã€ä»¥åŠå¯¹åº”çš„æ ‡å‡†å·®ï¼ˆç”¨äºè¯Šæ–­ç¨³å®šæ€§ï¼‰
    """
    tpls = load_eval_templates()
    means, stdevs = {}, {}

    def _do(key_tpl, out_key):
        m, s = _sample_scores(model_name, tpls[key_tpl], query, answer, samples, provider=provider)
        means[out_key], stdevs[out_key] = m, s

    _do("relevance", "relevance")
    _do("influence", "influence")
    _do("uniqueness", "uniqueness")
    _do("diversity", "diversity")
    _do("subjpos", "subjective_position")
    _do("subjcount", "subjective_count")
    _do("follow", "follow_up")

    print("TEMPLATES LOADED:", tpls.keys())

    return means, stdevs

def _subjective_to_0_100(subj: Dict[str, float]) -> float:
    """
    ä¸»è§‚ä¸ƒç»´å¹³å‡åˆ†ï¼ˆ0~20ï¼‰æ˜ å°„åˆ° 0~100ï¼š
    ä½ æŒ‡å®šçš„è§„åˆ™ï¼šæŸä¸€ç»´åŸå§‹æ‰“åˆ†ä¸º aï¼Œåˆ™
        score_0_100 = sqrt(5 * a) * 10
    - a å…è®¸ä¸ºå°æ•°ï¼Œä½†ä¼šè¢« clamp åˆ° [1, 5]
    - ä¾‹å¦‚ï¼š
        a = 1  â†’ score â‰ˆ sqrt(5) * 10 â‰ˆ 22
        a = 5  â†’ score = sqrt(25) * 10 = 50
    """
    keys = [
        "relevance",
        "influence",
        "uniqueness",
        "diversity",
        "subjective_position",
        "subjective_count",
        "follow_up",
    ]
    mapped_vals = []

    for k in keys:
        v = subj.get(k, 0.0)
        try:
            v = float(v)
        except Exception:
            # è§£æå¤±è´¥æ—¶æŒ‰ 1 åˆ†å¤„ç†ï¼ˆæå·®ï¼Œä½†ä¸æ˜¯ 0ï¼‰
            v = 1.0

        # é™å®šåœ¨ [1, 20] ä¹‹é—´
        if v <= 0.0:
            v = 1.0
        v = max(1.0, min(20.0, v))

        # æ ¹æ®ä½ æŒ‡å®šçš„å…¬å¼ï¼šscore_0_100 = sqrt(5 * a) * 10
        mapped = math.sqrt(5.0 * v) * 10.0
        mapped_vals.append(mapped)

    if not mapped_vals:
        return 0.0

    return float(mean(mapped_vals))


def compute_geo_score(subj: Dict[str, float], obj: Dict[str, float]) -> float:
    """
    æ€»åˆ† = ä¸»è§‚(ä¸ƒç»´å‡å€¼çš„0~100) + å®¢è§‚é™„åŠ é¡¹ï¼ˆæœ€å¤š+40ï¼‰
    """
    subjective = _subjective_to_0_100(subj)

    # å®¢è§‚åŠ åˆ†ï¼ˆå¯å‘å¼ä¸Šé™ 40ï¼‰
    cr = obj.get("compression_ratio", 1.0)
    ttr = obj.get("ttr", 0.0)              # 0~1
    fre = obj.get("reading_ease", 0.0)     # 0~100

    bonus = 0.0
    bonus += max(0.0, min(20.0, 20.0 * (1.0 - cr)))    # æ›´ç²¾ç‚¼æ›´åŠ åˆ†ï¼ˆä¸Šé™20ï¼‰
    bonus += max(0.0, min(10.0, fre / 10.0))           # å¯è¯»æ€§ï¼ˆä¸Šé™10ï¼‰
    bonus += max(0.0, min(10.0, ttr * 100.0 / 50.0))   # TTR=0.5 è®°æ»¡10åˆ†

    return float(max(0.0, min(100.0, subjective + bonus)))

def evaluate_geo_score(
    model_name: str,
    query: str,
    src_text: str,
    opt_text: str,
    provider: str = "auto",
    mode: Literal["single_text","with_citations"] = "single_text",
    samples: int = 1
) -> GeoScore:
    """
    å¯¹ä¼˜åŒ–ç¨¿ï¼ˆopt_textï¼‰è¿›è¡Œä¸»è§‚ä¸ƒç»´è¯„å®¡ + å®¢è§‚æŒ‡æ ‡è®¡ç®—ï¼Œè¿”å›ç»Ÿä¸€ç»“æ„ã€‚
    """
    t0 = time.time()
    subj_means, subj_std = evaluate_subjective_scores(
        model_name, query, opt_text, provider=provider, samples=samples
    )
    obj = {
        "compression_ratio": compression_ratio(src_text, opt_text),
        "ttr": type_token_ratio(opt_text),
        "reading_ease": reading_ease(opt_text, lang="auto"),
    }
    total = compute_geo_score(subj_means, obj)
    dt = int((time.time() - t0) * 1000)

    return GeoScore(
        relevance=subj_means["relevance"],
        influence=subj_means["influence"],
        uniqueness=subj_means["uniqueness"],
        diversity=subj_means["diversity"],
        subjective_position=subj_means["subjective_position"],
        subjective_count=subj_means["subjective_count"],
        follow_up=subj_means["follow_up"],
        objective=obj,
        geo_score=total,
        mode=mode,
        model_used=f"{provider}:{model_name}",
        latency_ms=dt,
        samples=samples,
        stddev=subj_std
    )

def build_geo_summary(
    grade: str,
    sealed_overall: float | None,
    user_tier: str,
    raw_scores: Optional[Dict[str, float]] = None,
    objective: Optional[Dict[str, float]] = None,
) -> str:
    """
    æ„é€  GEO-Score çš„è‡ªç„¶è¯­è¨€ summaryã€‚

    - grade: A~E ç­‰çº§ï¼ˆæ¥è‡ª sealed_overallï¼‰
    - sealed_overall: 0~1 çš„ç»¼åˆæŒ‡æ•°
    - user_tier: 'free' | 'alpha' | 'pro' | 'debug'
    - raw_scores: 7 ç»´ä¸»è§‚æŒ‡æ ‡ï¼ˆ0~1ï¼‰
        fluency / coverage / relevance / uniqueness / diversity / authority / follow_up
    - objective: å®¢è§‚æŒ‡æ ‡ï¼ˆè¿™é‡Œåªä½¿ç”¨ ttr / reading_easeï¼Œå®Œå…¨ä¸è§£é‡Š CRï¼‰
    """
    # -------- overall index ----------
    if sealed_overall is None:
        idx = None
        idx_str = "â€“"
    else:
        idx = float(sealed_overall)
        idx_str = f"{idx * 100:.1f}"

    tier = (user_tier or "free").lower().strip()

    # -------- 7 ç»´ç»´åº¦æ˜ å°„ & åˆ†æ•°æ¡£ä½ ----------
    pretty_dim = {
        "fluency": "Fluency",
        "coverage": "Coverage",
        "relevance": "Pertinence",
        "uniqueness": "Distinctiveness",
        "diversity": "Variety",
        "authority": "Authority",
        "follow_up": "Pursue",
    }

    scores: Dict[str, float] = {}
    if raw_scores:
        for k, v in raw_scores.items():
            try:
                scores[k] = float(v)
            except Exception:
                continue

    def _band(v: float) -> str:
        """æŠŠ 0~1 çš„å¾—åˆ†åˆ‡æˆæ¡£ä½æ ‡ç­¾ã€‚"""
        if v >= 0.80:
            return "high"          # å¾ˆå¼º / è¡¨ç°çªå‡º
        if v >= 0.65:
            return "upper_mid"     # æ˜æ˜¾åå¥½
        if v >= 0.50:
            return "mid"           # ä¸­ç­‰
        if v >= 0.35:
            return "lower_mid"     # åå¼±
        return "low"               # è¾ƒå¼±

    def _pick_dim_lists():
        """é€‰å‡ºç›¸å¯¹è¡¨ç°é å‰/é åçš„ç»´åº¦åˆ—è¡¨ï¼Œå¹¶åŒºåˆ†æ˜¯å¦å­˜åœ¨â€œç»å¯¹å¼ºé¡¹â€."""
        if not scores:
            return [], [], False

        items = sorted(scores.items(), key=lambda kv: kv[1], reverse=True)
        top = items[:3]
        bottom = list(reversed(items))[:3]

        strong_dims = [k for k, v in top if v >= 0.65]
        weak_dims = [k for k, v in bottom if v <= 0.5]

        # å½“æ‰€æœ‰åˆ†æ•°éƒ½ä¸é«˜æ—¶ï¼Œç”¨â€œç›¸å¯¹ä¸é‚£ä¹ˆå¼±â€çš„è¯´æ³•ï¼Œè€Œä¸æ˜¯ strong/weak
        has_real_strength = any(scores[k] >= 0.7 for k in scores.keys())

        return strong_dims, weak_dims, has_real_strength

    strong_dims, weak_dims, has_real_strength = _pick_dim_lists()

    def _fmt_dim_list(keys):
        labels = [pretty_dim.get(k, k) for k in keys]
        if not labels:
            return ""
        if len(labels) == 1:
            return labels[0]
        if len(labels) == 2:
            return f"{labels[0]} and {labels[1]}"
        return ", ".join(labels[:-1]) + " and " + labels[-1]

    # -------- ç»´åº¦çº§åˆ«ï¼šname + score çš„è§£é‡Šï¼ˆä¸»è¦ç”¨äº Proï¼‰ ----------
    def _dim_comment(key: str, v: float) -> str:
        label = pretty_dim.get(key, key)
        b = _band(v)

        if key == "relevance":
            if b == "high":
                return f"{label} is high: the draft stays closely aligned with the user question."
            if b == "upper_mid":
                return f"{label} is clearly above average, mostly staying on topic with only minor drift."
            if b == "mid":
                return f"{label} is in a middle band: the draft generally matches the question but occasionally drifts."
            if b == "lower_mid":
                return f"{label} is on the weak side, with noticeable off-topic or under-explained parts."
            return f"{label} is low, meaning the draft often misses or only partially answers the core question."

        if key == "coverage":
            if b == "high":
                return f"{label} is high: key points are well covered within the space of the rewrite."
            if b == "upper_mid":
                return f"{label} is above average, covering most of the important aspects."
            if b == "mid":
                return f"{label} is moderate: some core aspects are present, but a few angles are underdeveloped."
            if b == "lower_mid":
                return f"{label} is relatively weak; the draft only touches a subset of the necessary points."
            return f"{label} is low, suggesting large gaps in what a reader would expect to see."

        if key == "uniqueness":
            if b == "high":
                return f"{label} is high: the wording and framing feel distinctive rather than generic."
            if b == "upper_mid":
                return f"{label} is above average, with a noticeable amount of original framing."
            if b == "mid":
                return f"{label} is in a neutral band; the draft feels serviceable but not particularly original."
            if b == "lower_mid":
                return f"{label} is on the low side, with the text feeling quite template-like."
            return f"{label} is low, making the draft look formulaic and hard to differentiate in GEO."

        if key == "diversity":
            if b == "high":
                return f"{label} is high: the draft uses varied structures and perspectives."
            if b == "upper_mid":
                return f"{label} is above average in {label.lower()}, giving the content a richer feel."
            if b == "mid":
                return f"{label} is moderate; the draft mostly repeats a few patterns."
            if b == "lower_mid":
                return f"{label} is relatively weak, with limited variety in examples or angles."
            return f"{label} is low, so the draft feels monotonous and easy to skim past."

        if key == "authority":
            if b == "high":
                return f"{label} is high: the draft feels grounded with clear signals of expertise or credible references."
            if b == "upper_mid":
                return f"{label} is above average, offering some evidence or expert framing."
            if b == "mid":
                return f"{label} is middling; the draft asserts claims but does not always back them with signals of trust."
            if b == "lower_mid":
                return f"{label} is on the weak side, with many claims sounding somewhat unsupported."
            return f"{label} is low, meaning the draft lacks cues that models and readers can treat as trustworthy."

        if key == "follow_up":
            if b == "high":
                return f"{label} is high: the draft naturally opens up clear next questions or actions."
            if b == "upper_mid":
                return f"{label} is above average, offering a few good hooks for follow-up."
            if b == "mid":
                return f"{label} is moderate; follow-up space exists but is not made explicit."
            if b == "lower_mid":
                return f"{label} is relatively weak, with few hints about what to ask or do next."
            return f"{label} is low, so the draft feels like a dead end rather than a step in an ongoing dialogue."

        if key == "fluency":
            if b == "high":
                return f"{label} is high: sentences read smoothly and are easy to parse."
            if b == "upper_mid":
                return f"{label} is above average, with mostly natural flow."
            if b == "mid":
                return f"{label} is in a middle band; some phrases may feel a bit stiff or dense."
            if b == "lower_mid":
                return f"{label} is relatively weak and may slow readers down."
            return f"{label} is low, making the draft feel heavy or awkward to read."

        # fallback ä¸€èˆ¬è¯´æ˜
        if b == "high":
            return f"{label} is high for this draft."
        if b == "upper_mid":
            return f"{label} is clearly above average."
        if b == "mid":
            return f"{label} is in a middle band."
        if b == "lower_mid":
            return f"{label} is on the weak side."
        return f"{label} is low and needs attention."

    # -------- å®¢è§‚æŒ‡æ ‡è§£æï¼šåªä½¿ç”¨ TTR + Reading Easeï¼Œå®Œå…¨ä¸è§£é‡Š CR ----------
    ttr = None
    fre = None
    if objective:
        try:
            if objective.get("ttr") is not None:
                ttr = float(objective["ttr"])
        except Exception:
            ttr = None
        try:
            if objective.get("reading_ease") is not None:
                fre = float(objective["reading_ease"])
        except Exception:
            fre = None

    def _ttr_phrase(short: bool = False) -> str:
        if ttr is None:
            return ""
        if ttr >= 0.55:
            return "uses fairly diverse wording" if short else (
                "The wording is fairly diverse, which helps the text feel less repetitive."
            )
        if ttr >= 0.35:
            return "keeps a balanced level of wording variety" if short else (
                "The text keeps a balanced level of wording variety, which is usually comfortable for readers and models."
            )
        return "relies on rather repetitive wording" if short else (
            "The text relies on rather repetitive wording, which can make it feel mechanical or generic."
        )

    def _fre_phrase_short() -> str:
        if fre is None:
            return ""
        if fre >= 70:
            return "reads very easily"
        if fre >= 55:
            return "is reasonably easy to read"
        if fre >= 40:
            return "is somewhat dense to read"
        return "feels quite heavy and effortful to read"

    def _fre_phrase_long() -> str:
        if fre is None:
            return ""
        if fre >= 70:
            return "The reading ease score is high, so the draft should feel light and easy for most readers."
        if fre >= 55:
            return "The reading ease score is above average, and most readers can follow the text without much effort."
        if fre >= 40:
            return "The reading ease score is in a middle band: understandable, but some readers may find it a bit dense."
        return "The reading ease score is low, so the draft may feel heavy or cognitively demanding; simplifying sentences would help."

    # -------- Free tierï¼šæ•´ä½“+ç›¸å¯¹è¡¨ç°+å¼•å¯¼å‡çº§ ----------
    if tier == "free":
        base = f"GEO-Max rated this draft at grade {grade}"
        if idx_str != "â€“":
            base += "."
        else:
            base += "."

        detail_parts: list[str] = []

        if scores:
            avg_score = sum(scores.values()) / len(scores)
            if avg_score >= 0.7:
                detail_parts.append("Overall the seven GEO dimensions are in a relatively strong band.")
            elif avg_score >= 0.5:
                detail_parts.append("Overall the seven GEO dimensions sit in a mid band.")
            else:
                detail_parts.append("Overall the seven GEO dimensions are on the weak side and would benefit from a focused revision.")

            if has_real_strength and strong_dims:
                detail_parts.append(
                    f"Within this profile, { _fmt_dim_list(strong_dims) } stand out as relatively better-performing dimensions."
                )
            if weak_dims:
                detail_parts.append(
                    f"{ _fmt_dim_list(weak_dims) } come out as the more constrained dimensions right now."
                )

        tail = " In the free preview you see only a overall index and a coarse dimension profile; full diagnostics are available in GEO Tools Alpha and Pro."
        return base + (" " + " ".join(detail_parts) if detail_parts else "") + tail

    # -------- Alpha tierï¼šä¸­ç­‰è¯¦ç»†åº¦è§£é‡Š + TTR/Reading Ease ç®€è¦æ–¹å‘ ----------
    if tier == "alpha":
        base = f"GEO-Max rated this draft at grade {grade}"
        if idx_str != "â€“":
            base += "."
        else:
            base += "."

        parts: list[str] = []

        if scores:
            avg_score = sum(scores.values()) / len(scores)
            if avg_score >= 0.7:
                parts.append("Overall the 7 GEO dimensions are in a fairly strong band for this draft.")
            elif avg_score >= 0.5:
                parts.append("Overall the 7 GEO dimensions are in a middle band.")
            else:
                parts.append("Overall the 7 GEO dimensions are on the weaker side and would benefit from targeted polishing.")

            if has_real_strength and strong_dims:
                parts.append(
                    f"{ _fmt_dim_list(strong_dims) } are the relatively better dimensions."
                )
            if weak_dims:
                parts.append(
                    f"{ _fmt_dim_list(weak_dims) } are the dimensions where improvement would bring the most gain."
                )

        obj_phrases: list[str] = []
        ttr_s = _ttr_phrase(short=True)
        if ttr_s:
            obj_phrases.append(ttr_s)
        fre_s = _fre_phrase_short()
        if fre_s:
            obj_phrases.append(fre_s)

        if obj_phrases:
            parts.append(
                "From the objective side, the text " + ", ".join(obj_phrases) + "."
            )

        tail = " This Alpha view already folds these objective signals into the score."
        return base + (" " + " ".join(parts) if parts else "") + tail

    # -------- Pro tierï¼šç»´åº¦+åˆ†æ•°è§£é‡Š + TTR/Reading Ease è¯¦ç»†è¯Šæ–­ ----------
    if tier == "pro":
        base = f"GEO-Max rated this draft at grade {grade}"
        if idx_str != "â€“":
            base += "."
        else:
            base += "."

        dim_comments: list[str] = []
        if scores:
            explained_keys: set[str] = set()
            for k in strong_dims[:2]:
                if k in scores:
                    dim_comments.append(_dim_comment(k, scores[k]))
                    explained_keys.add(k)
            for k in weak_dims[:2]:
                if k in scores and k not in explained_keys:
                    dim_comments.append(_dim_comment(k, scores[k]))
                    explained_keys.add(k)

            if not dim_comments:
                mid_items = sorted(scores.items(), key=lambda kv: kv[1], reverse=True)[:2]
                for k, v in mid_items:
                    dim_comments.append(_dim_comment(k, v))

        obj_detail_parts: list[str] = []
        if ttr is not None:
            obj_detail_parts.append(
                f"The typeâ€“token ratio is about {ttr:.2f}; {_ttr_phrase(short=False)}"
            )
        if fre is not None:
            obj_detail_parts.append(
                f"The reading ease score is roughly {fre:.1f}/100. {_fre_phrase_long()}"
            )

        tail = " In Pro you get the full 7-dimension breakdown plus these objective diagnostics to guide precise revisions for GEO."

        return (
            base
            + (" " + " ".join(dim_comments) if dim_comments else "")
            + (" " + " ".join(obj_detail_parts) if obj_detail_parts else "")
            + " "
            + tail
        )

    # -------- Debug tierï¼šæœ€ç®€å•çš„è¯´æ˜ ----------
    if tier == "debug":
        base = f"GEO-Max rated this draft at grade {grade}"
        if idx_str != "â€“":
            base += "."
        else:
            base += "."
        return (
            base
            + " This debug view exposes all 7 subjective dimensions and objective metrics (excluding CR from interpretation) for internal inspection and engine tuning."
        )

    # -------- å…œåº• ----------
    if idx_str != "â€“":
        return f"GEO-Max rated this draft at grade {grade} ."
    return f"GEO-Max rated this draft at grade {grade}."

def _build_anchored_query(user_question: str, article_title: str) -> str:
    """
    å°†æ ‡é¢˜/é—®é¢˜ä½œä¸ºè¯„åˆ†é”šç‚¹å†™å…¥ queryï¼Œæœ€å°ä¾µå…¥ã€‚
    - æœ‰ titleï¼šä¼˜å…ˆæä¾› title + question
    - æ—  titleï¼šé€€åŒ–ä¸ºåŸæ¥çš„ user_question
    """
    q = (user_question or "").strip()
    t = (article_title or "").strip()

    if t and q:
        return f"[Article Title]\n{t}\n\n[User Question]\n{q}".strip()
    if t and not q:
        return f"[Article Title]\n{t}".strip()
    return q

def _estimate_tokens_rough(text: Optional[str]) -> int:
    """
    ç²—ç•¥ token ä¼°ç®—ï¼ˆä¸å‰ç«¯ approxTokens æ€è·¯ä¸€è‡´å³å¯ï¼‰ï¼š
    - è‹±æ–‡ï¼šæŒ‰å•è¯æ•°è¿‘ä¼¼
    - ä¸­æ–‡ï¼šæŒ‰ä¸­æ–‡å­—ç¬¦æ•°è¿‘ä¼¼
    - æ··åˆï¼šä¸¤è€…ç›¸åŠ 
    æ³¨æ„ï¼šè¿™æ˜¯â€œé•¿åº¦ç½®ä¿¡åº¦â€ç”¨é€”ï¼Œä¸ç”¨äºç²¾ç¡®è®¡è´¹ã€‚
    """
    if not text:
        return 0
    t = str(text).strip()
    if not t:
        return 0

    # ä¸­æ–‡å­—ç¬¦ï¼ˆCJK Unified Ideographs ç­‰å¤§è‡´èŒƒå›´ï¼‰
    cjk_chars = re.findall(r"[\u4e00-\u9fff]", t)
    cjk_count = len(cjk_chars)

    # è‹±æ–‡/æ•°å­—å•è¯
    words = re.findall(r"[A-Za-z0-9]+(?:'[A-Za-z0-9]+)?", t)
    word_count = len(words)

    # ä¸€ä¸ªç»éªŒï¼šä¸­æ–‡ 1 å­—â‰ˆ1 tokenï¼ˆç²—ç•¥ï¼‰ï¼Œè‹±æ–‡ 1 è¯â‰ˆ1~1.3 tokenï¼ˆè¿™é‡Œå– 1ï¼‰
    return cjk_count + word_count


def _length_confidence_ct(tokens: int) -> float:
    """
    ä½ çš„ Ct å®šä¹‰ï¼ˆæŒ‰ä½ ç»™å‡ºçš„è§„åˆ™è½åœ°ï¼‰ï¼š
    - Hard Threshold: tokens < 400 -> Ct = 0.4
    - Growth: 400~1800 -> Ct = min(1, log(tokens-300)/log(1500))
      ï¼ˆå› ä¸º 1800-300=1500ï¼‰
    - Overload: tokens > 2500 -> Ct = 0.95ï¼ˆè½»å¾®ä¸‹è°ƒï¼‰
    """
    try:
        n = int(tokens)
    except Exception:
        n = 0

    if n < 400:
        return 0.4

    if n <= 1800:
        # é˜²æ­¢ log(<=0)
        x = max(1, n - 300)
        denom = math.log(1800 - 300)  # log(1500)
        if denom <= 0:
            return 1.0
        return min(1.0, math.log(x) / denom)

    # 1800 ä»¥ä¸Šé»˜è®¤æ»¡åˆ†ç³»æ•°
    ct = 1.0

    # è¿‡è½½åŒºé—´ï¼š>2500 è½»å¾®ä¸‹è°ƒ
    if n > 2500:
        ct = 0.95

    return ct

# ============================================================
# ğŸ” ç»Ÿä¸€æš´éœ²ç»™å¤–éƒ¨çš„ Geo æŒ‡æ•°å…¥å£ï¼ˆç»™ FastAPI / å‰ç«¯è°ƒç”¨ï¼‰
#      â€”â€” å¢åŠ ç¼“å­˜ï¼šåŒä¸€ç¯‡å†…å®¹ + æ¨¡å‹ï¼Œåªè¯„ä¸€æ¬¡ï¼Œå¤šè§†å›¾å°å°
# ============================================================

def geo_score_pipeline(
    user_question: str,
    article_title: str = "",
    source_text: str = "",
    rewritten_text: str = "",
    model_ui: str = "groq",
    model_name: str = "llama-3.3-70b-versatile",
    samples: int = 1,
    user_tier: str = "free",
):
    """
    GEO-Score å¯¹å¤–ç»Ÿä¸€å…¥å£ï¼š

    - è¾“å…¥ï¼šuser_question / article_title(å¯é€‰) / åŸæ–‡ / æ”¹å†™æ–‡ + æ¨¡å‹é…ç½® + user_tier
    - è¾“å‡ºï¼šå‰ç«¯ç›´æ¥ä½¿ç”¨çš„ geo_score / grade / summary / sealed è§†å›¾
    """
    start_ts = time.time()
    try:
        
        anchored_query = _build_anchored_query(user_question=user_question, article_title=article_title)

        # â­ è®¡ç®—æœ¬æ¬¡è¯„ä¼°çš„ keyï¼ˆå†…å®¹ + æ¨¡å‹ + providerï¼‰
        cache_key = _make_cache_key(
            model_ui=model_ui,
            model_name=model_name,
            user_question=user_question,
            article_title=article_title or "",
            source_text=source_text,
            rewritten_text=rewritten_text,
        )

        base = _GEO_BASE_CACHE.get(cache_key)

        # è¿™äº›å˜é‡åœ¨åç»­ä¼šç”¨åˆ°ï¼ˆä¿è¯æ— è®º cache hit/miss éƒ½å­˜åœ¨ï¼‰
        subj_0_1: Dict[str, float] = {}
        raw_scores_0_100: Dict[str, float] = {}

        if base is None:
            _cache_dbg("score", False, cache_key, {"provider": model_ui, "model": model_name})
            # ================================
            #  ç¼“å­˜æœªå‘½ä¸­ï¼šçœŸæ­£è·‘ä¸€éè¯„ä¼°
            # ================================
            result = evaluate_geo_score(
                model_name=model_name,
                query=anchored_query,    
                src_text=source_text,
                opt_text=rewritten_text,
                provider=model_ui,
                mode="single_text",
                samples=samples,
            )

            if hasattr(result, "dict"):
                raw: Dict[str, Any] = result.dict()
            elif isinstance(result, dict):
                raw = result
            else:
                raw = result.__dict__

            subj = {
                "relevance": float(raw.get("relevance", 0.0)),
                "influence": float(raw.get("influence", 0.0)),
                "uniqueness": float(raw.get("uniqueness", 0.0)),
                "diversity": float(raw.get("diversity", 0.0)),
                "subjective_position": float(raw.get("subjective_position", 0.0)),
                "subjective_count": float(raw.get("subjective_count", 0.0)),
                "follow_up": float(raw.get("follow_up", 0.0)),
            }
            obj = raw.get("objective") or {}
            geo_score_0_100 = float(raw.get("geo_score", 0.0))

            # ========= ä¸»è§‚ 1~20 â†’ 0~1 / 0~100ï¼ˆä¸ä½ æŒ‡å®šçš„å…¬å¼ä¸¥æ ¼å¯¹é½ï¼‰ =========
            def _scale_1_20_to_0_1(x: float) -> float:
                """
                ä¸»è§‚æ‰“åˆ† b âˆˆ [1,20] æ˜ å°„åˆ° 0~1ï¼ˆä¸ä½ ç¡®è®¤çš„ 1â€“20 ä½“ç³»ä¸¥æ ¼å¯¹é½ï¼‰ï¼š
                    score_0_100 = sqrt(5 * b) * 10
                    score_0_1   = score_0_100 / 100 = sqrt(5 * b) / 10
                å…³é”®é”šç‚¹ï¼ˆä¸æ—§ 1â€“5 å¯¹åº” 4/8/12/16/20 å®Œå…¨ç­‰ä»·ï¼‰ï¼š
                    b=4  â†’ 0_100â‰ˆ44.72 â†’ 0_1â‰ˆ0.4472
                    b=12 â†’ 0_100â‰ˆ77.46 â†’ 0_1â‰ˆ0.7746
                    b=20 â†’ 0_100=100   â†’ 0_1=1
                """
                try:
                    v = float(x)
                except Exception:
                    v = 1.0
                if v <= 0.0:
                    v = 1.0
                v = max(1.0, min(20.0, v))
                return math.sqrt(5.0 * v) / 10.0


            
            subj_0_1 = {k: _scale_1_20_to_0_1(v) for k, v in subj.items()}
            subj_0_100 = {k: float(v) * 100.0 for k, v in subj_0_1.items()}

            # ========= å®¢è§‚ reading_ease â†’ 0~1 =========
            def _scale_0_100_to_0_1(x: float) -> float:
                try:
                    v = float(x) / 100.0
                    return max(0.0, min(1.0, v))
                except Exception:
                    return 0.0

            reading_ease_val = obj.get("reading_ease", None)
            reading_ease_0_1 = (
                _scale_0_100_to_0_1(reading_ease_val)
                if reading_ease_val is not None
                else None
            )

            # Fluency å–å€¼ç­–ç•¥ï¼š
            # - ä¼˜å…ˆ reading_easeï¼ˆå¯è¯»æ€§ï¼‰
            # - ç¼ºå¤±/ä¸º0 åˆ™å›é€€åˆ° pertinenceï¼ˆrelevanceï¼‰
            if reading_ease_0_1 is not None and reading_ease_0_1 > 0.0:
                fluency_0_1 = reading_ease_0_1
            else:
                fluency_0_1 = subj_0_1["relevance"]

            coverage_0_1 = (
                subj_0_1["subjective_position"] + subj_0_1["subjective_count"]
            ) / 2.0

            raw_scores = {
                "fluency": float(fluency_0_1),
                "coverage": float(coverage_0_1),
                "relevance": float(subj_0_1["relevance"]),       # UI: Pertinence
                "uniqueness": float(subj_0_1["uniqueness"]),     # UI: Distinctiveness
                "diversity": float(subj_0_1["diversity"]),       # UI: Variety
                "authority": float(subj_0_1["influence"]),       # UI: Authority
                "follow_up": float(subj_0_1["follow_up"]),       # UI: Pursue
            }
            raw_scores_0_100 = {k: float(v) * 100.0 for k, v in raw_scores.items()}

            # âœ… å…³é”®ï¼šæŠŠ subj_0_1 / raw_scores_0_100 ä¸€å¹¶å¡è¿›ç¼“å­˜ï¼Œç¡®ä¿ cache hit ä¹Ÿèƒ½ debug
            base = {
                "raw": raw,
                "subj": subj,
                "subj_0_1": subj_0_1,                 # âœ… æ–°å¢
                "subj_0_100": subj_0_100,
                "objective": obj,
                "geo_score_0_100": geo_score_0_100,
                "raw_scores": raw_scores,
                "raw_scores_0_100": raw_scores_0_100,  # âœ… æ–°å¢
            }
            _GEO_BASE_CACHE[cache_key] = base

        else:
            _cache_dbg("score", True, cache_key, {"provider": model_ui, "model": model_name})
            # ================================
            #  ç¼“å­˜å‘½ä¸­ï¼šç›´æ¥å¤ç”¨ä¸Šä¸€æ¬¡è¯„ä¼°ç»“æœ
            # ================================
            raw = base["raw"]
            subj = base["subj"]
            subj_0_100 = base["subj_0_100"]
            obj = base["objective"]
            geo_score_0_100 = base["geo_score_0_100"]
            raw_scores = base["raw_scores"]

            # âœ… å…³é”®ï¼šsubj_0_1 åœ¨ç¼“å­˜é‡Œä¼˜å…ˆå–ï¼›æ²¡æœ‰å°±ç”± subj_0_100 æ´¾ç”Ÿï¼ˆé¿å…ä¸ºç©º/æŠ¥é”™ï¼‰
            subj_0_1 = base.get("subj_0_1") or {k: float(v) / 100.0 for k, v in (subj_0_100 or {}).items()}

            # âœ… raw_scores_0_100 åŒç†
            raw_scores_0_100 = base.get("raw_scores_0_100") or {k: float(v) * 100.0 for k, v in (raw_scores or {}).items()}

        # ================================
        # Length Confidence Coefficient (Ct)
        # - åœ¨â€œæ‰“å®Œåˆ†(raw_scores)â€ä¹‹åç»Ÿä¸€ä¹˜ç³»æ•°
        # - åœ¨ seal_metrics ä¹‹å‰åšï¼Œç¡®ä¿å±•ç¤º/overall ä¹Ÿè¢«å½±å“
        # ================================
        text_for_len = (rewritten_text or "").strip() or (source_text or "").strip()
        est_tokens = _estimate_tokens_rough(text_for_len)
        ct = _length_confidence_ct(est_tokens)

        # å¯¹ 7 ç»´é€é¡¹ä¹˜ Ctï¼Œå¹¶ clamp åˆ° [0, 1]
        raw_scores = {k: max(0.0, min(1.0, float(v) * ct)) for k, v in (raw_scores or {}).items()}
        raw_scores_0_100 = {k: float(v) * 100.0 for k, v in raw_scores.items()}


        # debug åœ¨å°å°å±‚æŒ‰ pro çš„è§†è§’è¾“å‡º 7 ç»´
        tier_for_seal = "pro" if user_tier == "debug" else user_tier
        sealed_view = seal_metrics(raw_scores, user_tier=tier_for_seal)

        sealed_overall = sealed_view.get("overall_score")
        if sealed_overall is None:
            sealed_overall = max(0.0, min(1.0, float(geo_score_0_100) / 100.0))
            sealed_view["overall_score"] = sealed_overall

        def _map_grade(idx_0_1: float) -> str:
            try:
                s = float(idx_0_1)
            except Exception:
                return "â€“"
            if s >= 0.85:
                return "A"
            if s >= 0.70:
                return "B"
            if s >= 0.55:
                return "C"
            if s >= 0.40:
                return "D"
            return "E"

        grade = _map_grade(sealed_overall)

        summary = build_geo_summary(
            grade=grade,
            sealed_overall=sealed_overall,
            user_tier=user_tier,
            raw_scores=raw_scores,
            objective=obj,
        )

        latency_ms = (time.time() - start_ts) * 1000.0

        raw_debug = {
            "subjective_raw_1_20": subj,
            "subjective_0_100": subj_0_100,
            "objective": obj,
            "geo_score_raw_0_100": geo_score_0_100,
        }

        out: Dict[str, Any] = {
            "ok": True,
            "error": None,
            "geo_score": geo_score_0_100,
            "grade": grade,
            "summary": summary,
            "subjective": subj,
            "objective": obj,
            "sealed": sealed_view,
            "sealed_overall": sealed_overall,
            "latency_ms": latency_ms,
            "model_used": raw.get("model_used"),
            "samples": raw.get("samples", samples),
            "user_tier": user_tier,
        }

        # âœ… ä½ å·²æ”¹è¿‡ï¼šdebug_on OR user_tier == "debug"ï¼ˆè¿™é‡Œç›´æ¥ç»™æœ€ç»ˆç‰ˆï¼‰
        debug_on = os.getenv("GEO_SCORE_DEBUG", "0").strip() == "1"
        want_debug_view = debug_on or (user_tier == "debug")

        if want_debug_view:
            out["raw_scores_0_1"] = raw_scores
            out["raw_scores_0_100"] = raw_scores_0_100
            out["subjective_raw_1_20"] = subj
            out["subjective_scaled_0_1"] = subj_0_1
            out["subjective_scaled_0_100"] = subj_0_100
            # æ³¨æ„ï¼šç¡®ä¿ ct / est_tokens åœ¨ä¸Šæ–‡å·²è®¡ç®—å‡ºæ¥
            out["length_confidence"] = {"ct": ct, "est_tokens": est_tokens}

        if user_tier == "debug":
            out["raw_debug"] = raw_debug

        return out

    except Exception as e:
        latency_ms = (time.time() - start_ts) * 1000.0
        return {
            "ok": False,
            "error": f"geo_score_pipeline error: {e}",
            "geo_score": 0.0,
            "grade": "E",
            "summary": "GEO-Score evaluation failed.",
            "subjective": {},
            "objective": {},
            "sealed": {"overall_score": 0.0, "metrics": []},
            "sealed_overall": 0.0,
            "latency_ms": latency_ms,
            "model_used": None,
            "samples": samples,
            "user_tier": user_tier,
        }



def build_geo_report(project_title: str,
                     query: str,
                     src_text: str,
                     opt_text: str,
                     score: dict) -> str:
    """
    Stage3ï¼šå°† GEO-Score + æ–‡æœ¬å‰åå¯¹æ¯”æ¸²æŸ“æˆå¯åˆ†äº« HTMLã€‚
    """
    return render_report_html(
        project_title=project_title,
        query=query,
        src_text=src_text,
        opt_text=opt_text,
        score=score,
    )
