# app.py â€” GEO-Max å¤šæ¨¡å‹æ–‡æœ¬ä¼˜åŒ–å¼•æ“ï¼ˆè¯„åˆ†å¢å¼ºç‰ˆ Â· æç®€UIï¼‰
import os, json, requests, re, uuid, textwrap, tempfile
from datetime import datetime
from typing import Dict, Any, Tuple, List

import logging, traceback, time, sys
from pathlib import Path

import gradio as gr
from openai import OpenAI

# === æœ¬åœ°æ¨¡å— ===
from geo_logger import log_run
from geo_evaluator import evaluate_geo_score
from geo_report import render_report_html
from geo_impression import (
    impression_word_count,
    impression_pos_count,
    impression_wordpos_count,
    compute_delta
)

    
# === è°ƒè¯•/æ—¥å¿—è®¾ç½® ===
DEBUG_GEO_COT = True  # ä¸´æ—¶æ‰“å¼€ï¼›å®šä½å®Œæˆåå¯ç½® False
_log_path = Path(__file__).with_name("geo_cot_debug.log")
logging.basicConfig(
    level=logging.DEBUG if DEBUG_GEO_COT else logging.INFO,
    format="%(asctime)s [%(levelname)s] %(message)s",
    handlers=[logging.StreamHandler(sys.stdout), logging.FileHandler(_log_path, encoding="utf-8")]
)
def dbg(tag, **kw):
    if not DEBUG_GEO_COT: 
        return
    safe = {k: (str(v)[:800] + "â€¦[trunc]" if isinstance(v, str) and len(v) > 800 else v) for k, v in kw.items()}
    logging.debug(f"[GEO-COT:{tag}] {safe}")

def log_exc(tag):
    logging.error(f"[GEO-COT:{tag}] EXC={traceback.format_exc()}")
    
def safe_progress(p, *args, **kwargs):
    """å®‰å…¨è°ƒç”¨ gr.Progressï¼›é¿å… if p: è§¦å‘ __len__ å¯¼è‡´ IndexError"""
    try:
        if p is not None:
            p(*args, **kwargs)
    except Exception as _e:
        # å¯é€‰ï¼šæ‰“å°ä¸€è¡Œè°ƒè¯•ï¼Œä¸å½±å“ä¸»æµç¨‹
        print("[PROGRESS-IGNORED]", repr(_e))


# --- PATCH START ---
import gradio_client.utils as grc_utils
def _safe_json_schema_to_python_type(schema, defs=None):
    try:
        if isinstance(schema, bool):
            return "Any"
        return grc_utils._json_schema_to_python_type(schema, defs)
    except Exception:
        return "Any"
grc_utils.json_schema_to_python_type = _safe_json_schema_to_python_type
# --- PATCH END ---

def render_cot_markdown(data: dict) -> str:
    """
    å°† GEO-CoT ç»“æœè½¬ä¸ºå¯è¯» Markdownï¼ˆä¼˜å…ˆæ¸²æŸ“ evidence_chain_v2ï¼‰
    æ˜¾ç¤ºç»“æ„ï¼š
      # é€»è¾‘é“¾
      # è¯æ®é“¾ï¼ˆæŒ‰èŠ‚ç‚¹ï¼‰
        ## <èŠ‚ç‚¹1>
        - æ¥æºç±»å‹ï¼š<source_type>
        - å¯å¤è¿°äº‹å®ï¼š<claim>
        - å¦‚ä½•éªŒè¯ï¼š<how_to_verify>
        - å»ºè®®èµ„äº§ï¼š<asset>
        > âš ï¸ è¯æ®ç¼ºå£ï¼š<gaps>
      # æ ‡é¢˜ï¼ˆä¸è¯æ®é“¾èŠ‚ç‚¹å¯¹åº”ï¼‰
    """
    if not isinstance(data, dict):
        return "> æš‚æ— æ•°æ®"

    def _s(x):  # å®‰å…¨å–å­—ç¬¦ä¸²
        return (x or "").strip()

    def _render_proof_v2(pf: dict) -> list[str]:
        lines = []
        st = _s(pf.get("source_type"))
        cl = _s(pf.get("claim"))
        hv = _s(pf.get("how_to_verify"))
        asst = _s(pf.get("asset"))
        gp = _s(pf.get("gaps"))
        if st:   lines.append(f"- æ¥æºç±»å‹ï¼š`{st}`")
        if cl:   lines.append(f"- å¯å¤è¿°äº‹å®ï¼š{cl}")
        if hv:   lines.append(f"- å¦‚ä½•éªŒè¯ï¼š{hv}")
        if asst: lines.append(f"- å»ºè®®èµ„äº§ï¼š{asst}")
        if gp:   lines.append(f"> âš ï¸ è¯æ®ç¼ºå£ï¼š{gp}")
        return lines

    def _render_proof_v1(ev: dict) -> list[str]:
        """å…¼å®¹æ—§ç‰ˆ evidence_chain -> ç®€è¦è½¬å†™ä¸º v2 é£æ ¼"""
        if not isinstance(ev, dict):
            return []
        lines = []
        # ç®€åŒ–å±•ç¤ºï¼šæŠŠå¸¸è§ key æ‘˜è¦åŒ–
        mapping_keys = ["official","category_tags","products","scenarios",
                        "media_refs","tech_specs","third_party","structure"]
        picked = [k for k in mapping_keys if _s(ev.get(k))]
        if picked:
            lines.append(f"- æ¥æºç±»å‹ï¼š`mixed(v1)`")
            # åˆæˆä¸€å¥å¯å¤è¿°äº‹å®çš„å ä½
            lines.append("- å¯å¤è¿°äº‹å®ï¼šè¯¥èŠ‚ç‚¹åŒ…å«å®˜æ–¹ç®€ä»‹/äº§å“/åª’ä½“/æŠ€æœ¯/ç¬¬ä¸‰æ–¹ç­‰å¤šæºè¯æ®ï¼ˆv1ï¼‰")
            lines.append("- å¦‚ä½•éªŒè¯ï¼šæŒ‰å­—æ®µåˆ°å¯¹åº”é¡µé¢æˆ–æ–‡æ¡£æ ¸å¯¹ï¼ˆAbout/äº§å“é¡µ/åª’ä½“é¡µ/ç™½çš®ä¹¦/ç™¾ç§‘ç­‰ï¼‰")
            lines.append("- å»ºè®®èµ„äº§ï¼šAboutã€FAQã€HowToã€Product JSON-LDã€å¯¹æ¯”è¡¨")
        gaps = ev.get("gaps")
        if isinstance(gaps, list) and gaps:
            lines.append(f"> âš ï¸ è¯æ®ç¼ºå£ï¼š{'ï¼›'.join(str(x) for x in gaps)}")
        elif isinstance(gaps, str) and gaps.strip():
            lines.append(f"> âš ï¸ è¯æ®ç¼ºå£ï¼š{gaps.strip()}")
        return lines

    md = []

    # 1) é€»è¾‘é“¾
    logic = data.get("logic_chain") or []
    md.append("# ğŸ§  é€»è¾‘é“¾")
    if logic:
        for i, step in enumerate(logic, 1):
            md.append(f"{i}. {step}")
    else:
        md.append("- ï¼ˆç©ºï¼‰")

    # 2) è¯æ®é“¾ï¼ˆä¼˜å…ˆ v2ï¼‰
    md.append("\n# ğŸ”— è¯æ®é“¾ï¼ˆæŒ‰èŠ‚ç‚¹ï¼‰")
    ev2 = data.get("evidence_chain_v2") or []
    ev1 = data.get("evidence_chain") or []

    if ev2:
        for i, item in enumerate(ev2, 1):
            node = _s(item.get("node")) or f"èŠ‚ç‚¹{i}"
            proof = item.get("proof") or {}
            md.append(f"\n## {i}. {node}")
            lines = _render_proof_v2(proof)
            md.extend(lines if lines else ["- ï¼ˆè¯¥èŠ‚ç‚¹æš‚æ— å¯å±•ç¤ºå­—æ®µï¼‰"])
    elif ev1:
        # ä»…å½“æ²¡æœ‰ v2 æ—¶æ‰é™çº§å±•ç¤º v1
        for i, item in enumerate(ev1, 1):
            node = _s(item.get("node")) or f"èŠ‚ç‚¹{i}"
            md.append(f"\n## {i}. {node}")
            ev = item.get("evidence") or {}
            lines = _render_proof_v1(ev)
            md.extend(lines if lines else ["- ï¼ˆè¯¥èŠ‚ç‚¹æš‚æ— å¯å±•ç¤ºå­—æ®µï¼‰"])
    else:
        md.append("- ï¼ˆç©ºï¼‰")

    # 3) æ ‡é¢˜ï¼ˆä¸è¯æ®é“¾èŠ‚ç‚¹å¯¹åº”ï¼‰
    md.append("\n# ğŸ·ï¸ æ ‡é¢˜ï¼ˆä¸è¯æ®é“¾èŠ‚ç‚¹å¯¹åº”ï¼‰")
    titles = data.get("titles_by_node") or []
    # å»ºç«‹ node -> titles æ˜ å°„
    node2titles = {}
    for t in titles:
        n = _s(t.get("node"))
        arr = t.get("titles") or []
        if n:
            node2titles[n] = [str(x) for x in arr if _s(x)]

    # ä»¥â€œå·²å±•ç¤ºçš„è¯æ®é“¾é¡ºåºâ€ä¸ºå‡†è¾“å‡ºæ ‡é¢˜ï¼›è‹¥è¯æ®ä¸ºç©ºåˆ™æŒ‰é€»è¾‘é“¾é¡ºåº
    order_nodes = []
    if ev2:
        order_nodes = [ _s(x.get("node")) or f"èŠ‚ç‚¹{i+1}" for i, x in enumerate(ev2) ]
    elif ev1:
        order_nodes = [ _s(x.get("node")) or f"èŠ‚ç‚¹{i+1}" for i, x in enumerate(ev1) ]
    elif logic:
        order_nodes = list(logic)

    if order_nodes:
        any_title = False
        for i, node in enumerate(order_nodes, 1):
            arr = node2titles.get(node, [])
            if arr:
                any_title = True
                md.append(f"\n## {i}. {node}")
                for s in arr:
                    md.append(f"- {s}")
        if not any_title:
            md.append("- ï¼ˆè¯æ®é“¾èŠ‚ç‚¹æœªæ‰¾åˆ°å¯¹åº”æ ‡é¢˜ï¼‰")
    else:
        md.append("- ï¼ˆç©ºï¼‰")

    # ä¸åœ¨é˜…è¯»è§†å›¾é‡Œæ¸²æŸ“ raw_textï¼ˆä»…è°ƒè¯•æ¡†æ˜¾ç¤ºï¼‰
    return "\n".join(md)




def export_json_file(data: dict, filename: str = "geo_cot_export.json") -> str:
    """æŠŠ JSON è½æˆä¸´æ—¶æ–‡ä»¶ï¼Œè¿”å›è·¯å¾„ä¾› DownloadButton ä½¿ç”¨"""
    try:
        tmpdir = tempfile.gettempdir()
        path = os.path.join(tmpdir, filename)
        with open(path, "w", encoding="utf-8") as f:
            json.dump(data, f, ensure_ascii=False, indent=2)
        return path
    except Exception as e:
        return ""

_CIT_RE = re.compile(r"\[\s*(\d+)\s*\]")

# ===== ä¸»é¢˜ä¸ CSSï¼ˆåªåšè§†è§‰å±‚ï¼Œé›¶ä¾µå…¥ä¸šåŠ¡ï¼‰ =====
APP_THEME = gr.themes.Monochrome(
    primary_hue="indigo", secondary_hue="slate"
).set(
    button_primary_background_fill="linear-gradient(180deg,#6366f1,#4f46e5)",
    button_primary_background_fill_hover="linear-gradient(180deg,#4f46e5,#4338ca)",
    button_primary_text_color="#fff"
)

APP_CSS = """
/* ç»Ÿä¸€æµ…ç°èƒŒæ™¯ï¼Œç§»é™¤å‰²è£‚æ„Ÿ */
html, body { height:100%; background:#f5f7fb; color:#0f172a; }
.gradio-container { background:transparent !important;
  font:16px/1.72 -apple-system,BlinkMacSystemFont,Segoe UI,Roboto,Helvetica,Arial; }

/* ç‰ˆå¿ƒ 1280 */
#wrap{ max-width:1280px; margin:0 auto; padding:28px 20px 64px }

/* Tabsï¼šæ›´æ¸…æ™°çš„é€‰ä¸­çº¿ä¸æ‚¬åœ */
.tabs > div > button{
  border-bottom:2px solid transparent !important; border-radius:0 !important;
}
.tabs > div > button[aria-selected="true"]{
  border-bottom-color:#6366f1 !important; background:#ffffff !important;
}
.tabs > div > button:hover{ background:#eef1ff !important; }

/* ä¸¤åˆ—å¤–å£³ï¼štile å¡ç‰‡ */
.tile{
  background:#fff; border:1px solid #e7e9ee; border-radius:14px; padding:16px;
  box-shadow:0 1px 2px rgba(15,23,42,.04), 0 8px 22px rgba(15,23,42,.04);
}
.tile:hover{ box-shadow:0 2px 6px rgba(15,23,42,.06), 0 12px 32px rgba(15,23,42,.06) }

/* åˆ—å†…å‚ç›´é—´è· */
.stack > * + *{ margin-top:14px }

/* è¡¨å•èšç„¦æ€ */
textarea, input, select{
  border-radius:10px !important; border:1px solid #e6e8ef !important; background:#fff !important;
}
textarea:focus, input:focus, select:focus{
  outline:0 !important; border-color:#6366f1 !important; box-shadow:0 0 0 3px rgba(99,102,241,.18) !important;
}

/* æŒ‰é’®ï¼šé»˜è®¤ä¸ä¸»è¡ŒåŠ¨ */
.gradio-button{
  padding:11px 16px !important; border-radius:10px !important;
  border:1px solid #e7e9ee !important; background:#ffffff !important; color:#0f172a !important;
  transition:transform .12s ease, box-shadow .12s ease, background .12s ease;
}
.gradio-button:hover{ transform: translateY(-1px); box-shadow:0 3px 10px rgba(15,23,42,.08); }

.gradio-button.primary,
.gradio-button[data-testid="button-primary"]{
  background:linear-gradient(180deg,#6366f1,#4f46e5) !important; color:#fff !important; border:none !important;
  box-shadow:0 6px 16px rgba(79,70,229,.30);
}
.gradio-button.primary:hover,
.gradio-button[data-testid="button-primary"]:hover{
  transform: translateY(-1px); box-shadow:0 8px 24px rgba(79,70,229,.34);
}
.gradio-button.primary:active,
.gradio-button[data-testid="button-primary"]:active{
  transform: translateY(0); box-shadow:0 4px 12px rgba(79,70,229,.28);
}

/* è¡Œé—´è·ï¼šåˆ—æ›´ç–ä¸€äº› */
.gradio-row{ gap:24px !important; }

/* è½»é‡è„šæ³¨ */
.footnote{ margin-top:10px; color:#6b7280; font-size:12px; }

/* éšè— â€œBuilt with Gradioâ€ é¡µè„š */
footer, #footer, .gradio-container .footer, .built-with, .svelte-1ipelgc { display:none !important; }
"""


def _first_choice(choices):
    """ç»Ÿä¸€å®‰å…¨åœ°å–ç¬¬ä¸€æ¡ choiceï¼›æ— åˆ™è¿”å› Noneã€‚å…¼å®¹ list/tuple/pydantic å¯¹è±¡/Noneã€‚"""
    try:
        if not choices:
            return None
        # å…¼å®¹ pydantic/SDK å¯¹è±¡ï¼šä¼˜å…ˆç”¨è¿­ä»£å™¨
        it = iter(choices)
        return next(it, None)
    except Exception:
        # æŸäº›å¯¹è±¡å¯ __len__ ä½†ä¸å¯è¿­ä»£ï¼›é€€å›ç´¢å¼•å¹¶ä¿æŠ¤è¶Šç•Œ
        try:
            return choices[0] if getattr(choices, "__len__", None) and len(choices) > 0 else None
        except Exception:
            return None

# ===== å¼•ç”¨è§„æ•´ï¼ˆè®ºæ–‡æ¨¡å¼éœ€è¦ï¼‰ =====
def _has_std_cite(text: str) -> bool:
    return bool(_CIT_RE.search(text))

def _needs_fix(text: str) -> bool:
    if "æ¥æºå¾…è¡¥" in text or "ã€" in text or "ï¼ˆ" in text:  # ä¸­æ–‡æ‹¬å·/å ä½
        return True
    ids = [int(m.group(1)) for m in _CIT_RE.finditer(text)]
    return bool(ids and sorted(set(ids)) != list(range(1, max(ids)+1)))

def normalize_citation_markers(text: str) -> str:
    t = re.sub(r"ã€\s*(\d+)\s*ã€‘", r"[\1]", text)
    t = re.sub(r"ï¼ˆ\s*(\d+)\s*ï¼‰", r"[\1]", t)
    if "æ¥æºå¾…è¡¥" in t and not _has_std_cite(t):  # æ— ç¼–å·ä½†æœ‰å ä½
        t = t.replace("æ¥æºå¾…è¡¥", "[1]")
    return t

def maybe_citation_enrich(text: str) -> Tuple[str, bool]:
    if _has_std_cite(text) and not _needs_fix(text):
        return text, False
    t = normalize_citation_markers(text)
    return t, True

def _stringify_keys(d: Dict[int, float]) -> Dict[str, float]:
    return {str(k): float(v) for k, v in (d or {}).items()}

def run_impression_single(answer_with_citations: str, n_sources, mode: str):
    txt = (answer_with_citations or "").strip()
    if not txt:
        return "âš ï¸ è¯·è¾“å…¥åŒ…å« [1][2]â€¦ çš„ç­”æ¡ˆæ–‡æœ¬ã€‚", {}
    try:
        n = int(n_sources or 1)
        mode_l = (mode or "WordPos").lower()
        if mode_l.startswith("wordpos"):
            dist, used = impression_wordpos_count(txt, n), "WordPos"
        elif mode_l.startswith("word"):
            dist, used = impression_word_count(txt, n), "Word"
        else:
            dist, used = impression_pos_count(txt, n), "Pos"
        dist_str = _stringify_keys(dist)
        if not dist_str:
            return "âš ï¸ æœªè§£æåˆ°ä»»ä½• [x] å¼•ç”¨ã€‚è¯·ç¡®è®¤æ–‡æœ¬é‡Œæœ‰ [1][2]â€¦ æ ‡æ³¨ï¼Œä¸” Nâ‰¥æœ€å¤§ç¼–å·ã€‚", {}
        return f"âœ… {used} åˆ†å¸ƒè®¡ç®—å®Œæˆï¼ˆå„ä»½é¢ç›¸åŠ =1ï¼‰ã€‚", dist_str
    except Exception as e:
        return f"âŒ è§£æå¤±è´¥ï¼š{e}", {}

def run_impression_delta(before_answer: str, after_answer: str, n_sources, target_idx, mode: str):
    before_txt = (before_answer or "").strip()
    after_txt  = (after_answer or "").strip()
    if not before_txt or not after_txt:
        return "âš ï¸ è¯·åŒæ—¶å¡«å†™ Before ä¸ After çš„â€œå¸¦å¼•ç”¨ç­”æ¡ˆâ€ã€‚", {}
    try:
        n = int(n_sources or 1); t = int(target_idx or 1)
        res = compute_delta(before_txt, after_txt, n, t, mode or "WordPos")
        res_out: Dict[str, Any] = {
            "mode": res.get("mode"),
            "n_sources": int(res.get("n_sources", n)),
            "target_idx": int(res.get("target_idx", t)),
            "dist_before": _stringify_keys(res.get("dist_before", {})),
            "dist_after": _stringify_keys(res.get("dist_after", {})),
            "delta": float(res.get("delta", 0.0)),
        }
        msg = f"âœ… ç›®æ ‡æ¥æº [{res_out['target_idx']}] çš„ {res_out['mode']} ä»½é¢æå‡ Î” = {res_out['delta']:+.4f}ï¼ˆAfter - Beforeï¼‰ã€‚"
        if not res_out["dist_before"] or not res_out["dist_after"]:
            msg = "âš ï¸ æœªè§£æåˆ° [x] å¼•ç”¨æˆ– N è®¾ç½®è¿‡å°ï¼Œè¯·ç¡®è®¤æ–‡æœ¬ä¸­å­˜åœ¨ [1][2]â€¦ ä¸” Nâ‰¥æœ€å¤§ç¼–å·ã€‚"
        return msg, res_out
    except Exception as e:
        return f"âŒ è®¡ç®—å¤±è´¥ï¼š{e}", {}

# -----------------------------
# ç»Ÿä¸€ä¿å­˜ç›®å½•
SAVE_DIR = os.path.join(os.path.expanduser("~"), "GEO-Reports")
os.makedirs(SAVE_DIR, exist_ok=True)
# -----------------------------
# === ä¸¤æ®µå¼ / ä¸€æ®µå¼ Prompt å¤–ç½®åŒ–ç‰ˆ ===
def _read_text_file(path: str) -> str:
    try:
        with open(path, "r", encoding="utf-8") as f:
            return f.read()
    except Exception:
        return ""

def load_md_prompt_file(name: str) -> str:
    """
    ä¼˜å…ˆä» geo_prompts_md/{name}.md è¯»å– Markdown Promptã€‚
    è‹¥ä¸å­˜åœ¨ï¼Œåˆ™å°è¯•è¯»å– geo_prompts/{name}.json çš„ template å­—æ®µã€‚
    å‡ä¸å­˜åœ¨æ—¶ï¼Œè¿”å›ç©ºä¸²ï¼Œäº¤ç”±ä¸Šå±‚ç”¨ fallback å…œåº•ã€‚
    """
    base_md = os.path.join(os.path.dirname(__file__), "geo_prompts_md")
    md_path = os.path.join(base_md, f"{name}.md")
    txt = _read_text_file(md_path).strip()
    if txt:
        return txt

    # å…¼å®¹ä½ åŸæœ‰ JSON æ¨¡æ¿ç›®å½•
    base_json = os.path.join(os.path.dirname(__file__), "geo_prompts")
    json_path = os.path.join(base_json, f"{name}.json")
    try:
        with open(json_path, "r", encoding="utf-8") as f:
            data = json.load(f)
        return (data.get("template") or "").strip()
    except Exception:
        return ""

# å…è®¸çš„å ä½ç¬¦ï¼ˆä¼šè¢«ä¿ç•™ä¸º {NAME}ï¼Œå…¶å®ƒå¤§æ‹¬å·å…¨éƒ¨è½¬ä¹‰æˆ {{ }}ï¼‰
_ALLOWED_KEYS = {"USER_QUESTION", "BRAND_BRIEF", "MUST_EXPOSE", "EXPO_HINT", "STAGE1_JSON"}

_ALLOWED_KEYS = {"USER_QUESTION", "BRAND_BRIEF", "MUST_EXPOSE", "EXPO_HINT", "STAGE1_JSON"}

def _fmt_prompt(template: str, **vars) -> str:
    """
    å®‰å…¨æ ¼å¼åŒ–ï¼šå…ˆæ•´ä½“è½¬ä¹‰èŠ±æ‹¬å·ï¼Œå†åè½¬ä¹‰å…è®¸å ä½ç¬¦ã€‚
    è¿™æ ·ä½ çš„ MD é‡Œå³ä½¿åŒ…å« JSON ä¾‹å­æˆ–å¤§æ‹¬å·ï¼Œä¹Ÿä¸ä¼šè§¦å‘ KeyErrorã€‚
    """
    if not isinstance(template, str):
        template = str(template or "")
    t = template.replace("{", "{{").replace("}", "}}")
    for key in _ALLOWED_KEYS:
        t = t.replace("{{" + key + "}}", "{" + key + "}")
    return t.format(**vars)

# åŠ è½½å¤–ç½®æ¨¡æ¿ï¼ˆå†…å®¹æ— éœ€åœ¨ JSON é‡Œæ‰‹å·¥åŠ åŒå¤§æ‹¬å·ï¼‰
PROMPT_STAGE1      = load_md_prompt_file("cot_stage1")
PROMPT_STAGE2      = load_md_prompt_file("cot_stage2")
PROMPT_SINGLE_PASS = load_md_prompt_file("cot_single")

# === ï¼ˆå¯é€‰ï¼‰æ¨¡æ¿å ä½ç¬¦æ£€æŸ¥ï¼šæ”¾åœ¨åŠ è½½ä¹‹åï¼ï¼ ===

for name, tmpl in {
    "cot_stage1": PROMPT_STAGE1,
    "cot_stage2": PROMPT_STAGE2,
    "cot_single": PROMPT_SINGLE_PASS
}.items():
    if not tmpl:
        logging.warning(f"[PROMPT CHECK] æ¨¡æ¿ {name} ä¸ºç©ºæˆ–æœªåŠ è½½")
        continue
    # å…ˆèµ°ä¸€éâ€œå®‰å…¨æ”¾å¼€â€çš„å¤„ç†ï¼Œé¿å…æŠŠ JSON ç¤ºä¾‹å½“å ä½ç¬¦
    _safe = _fmt_prompt(tmpl,
                        USER_QUESTION="x",
                        BRAND_BRIEF="x",
                        MUST_EXPOSE="x",
                        EXPO_HINT="x",
                        STAGE1_JSON="{}")
    unknown = [m for m in re.findall(r"\{([A-Za-z0-9_]+)\}", _safe)
               if m not in _ALLOWED_KEYS]
    if unknown:
        logging.warning(f"[PROMPT CHECK] æ¨¡æ¿ {name} å«æœªå£°æ˜å ä½ç¬¦: {unknown}")


# ====== COT Prompt åŠ è½½ï¼ˆä¼˜å…ˆè¯»å¤–éƒ¨æ–‡ä»¶ï¼‰ ======
def _load_cot_template(fname: str, fallback: str) -> str:
    """ç»Ÿä¸€åŠ è½½é€»è¾‘ï¼Œè‹¥å¤–éƒ¨ä¸å­˜åœ¨æˆ– template ä¸ºç©ºï¼Œåˆ™ä½¿ç”¨ fallback"""
    p = os.path.join(os.path.dirname(__file__), fname)
    try:
        with open(p, "r", encoding="utf-8") as f:
            obj = json.load(f)
        tpl = (obj.get("template") or "").strip()
        return tpl if tpl else fallback
    except Exception:
        return fallback

# â€”â€” é€šç”¨é€»è¾‘ç­–ç•¥ç‰ˆ Fallback â€”â€” #
_FALLBACK_STAGE1 = """ä½ æ˜¯ GEO ï¼ˆGenerative Engine Optimizationï¼‰å†…å®¹é¡¾é—®ã€‚
è¯·ç”Ÿæˆä¸€ä¸ªåŒ…å« 3â€“6 æ­¥çš„â€œé€»è¾‘é“¾ï¼ˆlogic_chainï¼‰â€ï¼Œç”¨äºæè¿°ä¸€ä¸ªä»â€œå®šä¹‰é—®é¢˜â€åˆ°â€œå»ºç«‹éªŒè¯æœºåˆ¶â€çš„å®Œæ•´ GEO å†…å®¹ç­–ç•¥è¿‡ç¨‹ã€‚
ä»…è¾“å‡º JSONï¼š
{
  "logic_chain": ["æ­¥éª¤1","æ­¥éª¤2","æ­¥éª¤3", "æ­¥éª¤4", "æ­¥éª¤5"]
}
ã€è¾“å…¥ã€‘
- ç›®æ ‡é—®é¢˜ï¼š{USER_QUESTION}
- ç”²æ–¹èµ„æ–™ï¼š{BRAND_BRIEF}
- æœŸæœ›éœ²å‡ºï¼š{MUST_EXPOSE}
"""

_FALLBACK_STAGE2 = """ä½ æ˜¯ GEOï¼ˆGenerative Engine Optimizationï¼‰ å†…å®¹é¡¾é—®ã€‚
è¯·è¯»å–ä»¥ä¸‹ Stage1 ç»“æœï¼ˆlogic_chainï¼‰å¹¶ä¸ºæ¯ä¸ªèŠ‚ç‚¹ç”Ÿæˆ 1â€“2 æ¡â€œè¯æ®é“¾ï¼ˆevidence_chain_v2ï¼‰â€å’Œå¯¹åº”æ ‡é¢˜ã€‚
ä»…è¾“å‡º JSONï¼š
{
  "logic_chain": ["<å¤åˆ¶è‡ª Stage1>"],
  "evidence_chain_v2":[
    {"node":"<èŠ‚ç‚¹åç§°>",
     "proof":{"source_type":"official|product|media|user|research",
              "claim":"ä¸€å¥å¯éªŒè¯ä¸»å¼ ",
              "how_to_verify":"éªŒè¯æ–¹å¼",
              "asset":"å¯¹åº”äº§å‡ºæˆ–ç´ æ",
              "gaps":"å¾…è¡¥é¡¹æˆ–æ•°æ®ç¼ºå£"}}
  ],
  "titles_by_node":[{"node":"â€¦","titles":["â€¦","â€¦"]}]
}
ã€ä¸Šé˜¶æ®µ JSONã€‘
{STAGE1_JSON}
ã€æœŸæœ›éœ²å‡ºã€‘
{MUST_EXPOSE}
"""

_FALLBACK_SINGLE = """ä½ æ˜¯ GEO å†…å®¹é¡¾é—®ã€‚
è¯·åŸºäºè¾“å…¥ï¼Œç›´æ¥ç”Ÿæˆå®Œæ•´çš„â€œé€»è¾‘é“¾ + è¯æ®é“¾ï¼ˆv2ï¼‰+ å¯¹åº”æ ‡é¢˜â€ç»“æ„ã€‚
ä»…è¾“å‡º JSONï¼š
{
  "logic_chain":["èŠ‚ç‚¹1","èŠ‚ç‚¹2","èŠ‚ç‚¹3"],
  "evidence_chain_v2":[
    {"node":"èŠ‚ç‚¹1","proof":{"source_type":"","claim":"","how_to_verify":"","asset":"","gaps":""}}
  ],
  "titles_by_node":[{"node":"èŠ‚ç‚¹1","titles":["",""]}]
}
ã€è¾“å…¥ã€‘
- ç›®æ ‡é—®é¢˜ï¼š{USER_QUESTION}
- ç”²æ–¹èµ„æ–™ï¼š{BRAND_BRIEF}
- æœŸæœ›éœ²å‡ºï¼š{MUST_EXPOSE}
"""

def get_cot_prompts(
    user_q: str,
    brand_brief: str,
    must_expose: str,
    expo_hint: str = "",
    mode: str = "two-stage",
    stage1_json: dict | None = None,
):
    """
    ä½¿ç”¨ Markdown Promptï¼ˆgeo_prompts_md/*.mdï¼‰æ„é€ æç¤ºè¯ã€‚
    - single: è¿”å› p_singleï¼ˆåˆå¹¶è¾“å‡ºï¼‰
    - two-stage: è¿”å› p1ï¼ˆé˜¶æ®µ1ï¼‰ï¼Œp2ï¼ˆé˜¶æ®µ2ï¼‰
    è‹¥ MD ä¸å­˜åœ¨ï¼Œå°†ä¼˜å…ˆå›é€€åˆ° geo_prompts/*.json çš„ template å­—æ®µï¼›å†å›é€€åˆ° fallbackã€‚
    """
    user_q = (user_q or "").strip()
    brand_brief = (brand_brief or "").strip()
    must_expose = (must_expose or "").strip()
    expo_hint = (expo_hint or "").strip()

    # 1) åŠ è½½ MDï¼ˆæˆ– JSON / fallbackï¼‰
    stage1_tpl = load_md_prompt_file("cot_stage1") or _FALLBACK_STAGE1
    stage2_tpl = load_md_prompt_file("cot_stage2") or _FALLBACK_STAGE2
    single_tpl = load_md_prompt_file("cot_single") or _FALLBACK_SINGLE

    if str(mode).lower().startswith("single"):
        p_single = _fmt_prompt(
            single_tpl,
            USER_QUESTION=user_q,
            BRAND_BRIEF=brand_brief,
            MUST_EXPOSE=must_expose,
            EXPO_HINT=expo_hint,
        )
        return None, None, p_single

    # ä¸¤æ®µå¼
    p1 = _fmt_prompt(
        stage1_tpl,
        USER_QUESTION=user_q,
        BRAND_BRIEF=brand_brief,
        MUST_EXPOSE=must_expose,
        EXPO_HINT=expo_hint,
    )

    s1_json_text = json.dumps(stage1_json or {}, ensure_ascii=False, indent=2)
    p2 = _fmt_prompt(
        stage2_tpl,
        USER_QUESTION=user_q,
        BRAND_BRIEF=brand_brief,
        MUST_EXPOSE=must_expose,
        EXPO_HINT=expo_hint,
        STAGE1_JSON=s1_json_text,
    )
    return p1, p2, None

def export_md_file(text: str, filename: str = "geo_cot_output.md") -> str:
    try:
        tmpdir = tempfile.gettempdir()
        path = os.path.join(tmpdir, filename)
        with open(path, "w", encoding="utf-8") as f:
            f.write(text or "")
        return path
    except Exception:
        return ""



# ============ Prompt æ¨¡å—åŒ– ============
PROMPTS = {}
def load_prompts():
    path = os.path.join(os.path.dirname(__file__), "geo_prompts.json")
    global PROMPTS
    if not os.path.exists(path):
        PROMPTS["geo_max_zh"] = (
            "ä½ æ˜¯ä¸€åç”Ÿæˆå¼å¼•æ“ä¼˜åŒ–ï¼ˆGEOï¼‰ä¸“å®¶ã€‚è¯·èåˆä»¥ä¸‹9ç§ç­–ç•¥ï¼Œå¯¹ä¸‹åˆ—æ–‡æœ¬è¿›è¡Œç»¼åˆä¼˜åŒ–ï¼š"
            "1) æµç•…ä¼˜åŒ–,è°ƒæ•´å¥æ³•ç»“æ„ï¼Œä½¿å¥å­è‡ªç„¶é¡ºç•…ã€é€»è¾‘é€’è¿›ï¼›2) è¯æ±‡å¤šæ ·åŒ–,é¿å…é‡å¤ä½¿ç”¨åŒä¸€åŠ¨è¯æˆ–å½¢å®¹è¯ï¼›3) æƒå¨è¯­æ°”,å†…å®¹åº”ä½“ç°ä¸“ä¸šåˆ¤æ–­ã€åŸºäºäº‹å®ï¼›4) å¼•è¯­ï¼›5) å¼•ç”¨æ ‡è®°ï¼›6) ç®€æ´è¡¨è¾¾ï¼›"
            "7) æœ¯è¯­å¹¶è§£é‡Šï¼›8) æ•°æ®åŒ–æè¿°ï¼›9) å…³é”®è¯å¢å¼ºã€‚åªè¾“å‡ºä¼˜åŒ–æ­£æ–‡ã€‚\n---\nåŸæ–‡ï¼š\n{TEXT}\n---"
        )
    else:
        with open(path, "r", encoding="utf-8") as f:
            PROMPTS.update(json.load(f))

def build_geo_prompt(original_text: str) -> str:
    tpl = PROMPTS.get("geo_max_zh", "è¯·ä¼˜åŒ–ä»¥ä¸‹æ–‡æœ¬ï¼š{TEXT}")
    return tpl.replace("{TEXT}", original_text.strip())

# ====== æ–‡æœ¬åˆ†å— ======
DEFAULT_MAX_CHARS = 2800
def _split_to_units(text: str):
    seps = "ã€‚ï¼ï¼Ÿ!?ï¼."
    units = []
    for para in text.split("\n"):
        para = para.strip()
        if not para: continue
        buf = ""
        for ch in para:
            buf += ch
            if ch in seps:
                units.append(buf.strip()); buf = ""
        if buf.strip(): units.append(buf.strip())
        units.append("\n")
    while units and units[-1] == "\n": units.pop()
    return units or [text]

def chunk_text(text: str, max_chars: int = DEFAULT_MAX_CHARS):
    units = _split_to_units(text)
    chunks, cur = [], ""
    for u in units:
        if len(u) > max_chars:
            if cur: chunks.append(cur); cur = ""
            for i in range(0, len(u), max_chars): chunks.append(u[i:i+max_chars])
            continue
        if len(cur) + len(u) <= max_chars: cur += u
        else:
            if cur: chunks.append(cur)
            cur = u
    if cur: chunks.append(cur)
    return chunks

# ============ æ¨¡å‹é€‚é…ï¼ˆä½ ç°æœ‰çš„ä¸‰ä¸ªï¼‰ ============
def call_tongyi(prompt: str, timeout: int = 90) -> str:
    api_key = os.getenv("DASHSCOPE_API_KEY", "")
    if not api_key: return "âš ï¸ æœªé…ç½® DASHSCOPE_API_KEY ç¯å¢ƒå˜é‡ã€‚"
    try:
        client = OpenAI(api_key=api_key, base_url="https://dashscope.aliyuncs.com/compatible-mode/v1")
        completion = client.chat.completions.create(
            model="qwen3-max",
            messages=[{"role":"system","content":"You are a helpful assistant for text rewriting."},
                      {"role":"user","content":prompt}],
            temperature=0.7, timeout=timeout
        )
        
        choices = getattr(completion, "choices", None)
        first = _first_choice(choices)
        if first is None:
            return f"âš ï¸ é€šä¹‰è¿”å›ç©ºç»“æœï¼š{getattr(completion, 'model', 'unknown_model')}"
        msg = getattr(first, "message", None)

        content = getattr(msg, "content", None)
        if not content:
            return "âš ï¸ é€šä¹‰è¿”å›æ— å†…å®¹ï¼ˆmessage.content ä¸ºç©ºï¼‰ã€‚"
        return content.strip()
    except Exception as e:
        return f"âŒ é€šä¹‰åƒé—®è¯·æ±‚å¤±è´¥ï¼š{e}"

def call_deepseek(prompt: str, timeout: int = 90, model: str = "deepseek-v3.2-exp") -> str:
    api_key = os.getenv("DASHSCOPE_API_KEY") or os.getenv("TONGYI_API_KEY", "")
    if not api_key: return "âš ï¸ æœªé…ç½® DASHSCOPE_API_KEYï¼ˆæˆ– TONGYI_API_KEYï¼‰ã€‚"
    url = "https://dashscope.aliyuncs.com/compatible-mode/v1/chat/completions"
    headers = {"Authorization": f"Bearer {api_key}", "Content-Type": "application/json"}
    payload = {
        "model": model, 
        "messages":[
            {"role":"system","content":"You are a helpful assistant for text rewriting."},
            {"role":"user","content":prompt}
            ],
            "stream": False,
            "temperature": 0.7,
    }
    try:
        r = requests.post(url, headers=headers, json=payload, timeout=timeout)
        if not r.ok: return f"âŒ DeepSeekHTTPé”™è¯¯ {r.status_code}: {r.text}"
        
        data = r.json()
        choices = data.get("choices", None)
        first = _first_choice(choices)
        if first is None:
            return f"âš ï¸ DeepSeek è¿”å›ç©ºç»“æœï¼š{data}"
        # å…¼å®¹å¯¹è±¡/å­—å…¸ä¸¤ç§å½¢æ€
        msg = getattr(first, "message", None)
        if msg is None and isinstance(first, dict):
            msg = first.get("message", {})
        if msg is None:
            return f"âš ï¸ DeepSeek è¿”å›æ— å¯ç”¨ messageï¼š{data}"

        
        content = (msg.get("content") or "").strip()
        return content if content else "âš ï¸ DeepSeek è¿”å›æ— å†…å®¹ï¼ˆmessage.content ä¸ºç©ºï¼‰ã€‚"
    except Exception as e:
        return f"âŒ DeepSeek è¯·æ±‚å¤±è´¥ï¼š{e}"

def call_wenxin(prompt: str, timeout: int = 60) -> str:
    access_token = os.getenv("WENXIN_ACCESS_TOKEN", "")
    if not access_token: return "âš ï¸ æœªé…ç½® WENXIN_ACCESS_TOKENã€‚"
    url = f"https://aip.baidubce.com/rpc/2.0/ai_custom/v1/wenxinworkshop/chat/completions?access_token={access_token}"
    headers = {"Content-Type": "application/json"}
    payload = {"messages":[{"role":"system","content":"You are a helpful assistant for text rewriting."},
                           {"role":"user","content":prompt}],
               "temperature":0.7}
    try:
        r = requests.post(url, headers=headers, json=payload, timeout=timeout)
        r.raise_for_status()
        data = r.json()
        if "error_code" in data and data["error_code"] != 0:
            return f"âŒ æ–‡å¿ƒé”™è¯¯ï¼š{data.get('error_msg','unknown')}"
        return data.get("result","").strip()
    except Exception as e:
        return f"âŒ æ–‡å¿ƒä¸€è¨€è¯·æ±‚å¤±è´¥ï¼š{e}"

# ============ ä¸»æ¨ç†ï¼ˆç”Ÿæˆä¼˜åŒ–ç¨¿ï¼‰ ============
def run_geo(text: str, model_name: str, use_chunk: bool = True,
            max_chars: int = DEFAULT_MAX_CHARS, progress=gr.Progress(track_tqdm=True)):
    if not PROMPTS: load_prompts()
    if not text.strip(): return "âš ï¸ è¯·è¾“å…¥éœ€è¦ä¼˜åŒ–çš„æ–‡æœ¬ã€‚", text
    chunks = [text] if not use_chunk else chunk_text(text, max(800, int(max_chars)))
    total = len(chunks); outputs = []
    for idx, chunk in enumerate(chunks, start=1):
        progress((idx-1)/total, desc=f"å¤„ç†ä¸­ {idx}/{total} ...")
        prompt = build_geo_prompt(chunk)
        if model_name == "é€šä¹‰åƒé—®": out = call_tongyi(prompt)
        elif model_name == "DeepSeek": out = call_deepseek(prompt)
        elif model_name == "æ–‡å¿ƒä¸€è¨€": out = call_wenxin(prompt)
        else: out = "âš ï¸ æœªé€‰æ‹©æ¨¡å‹ã€‚"
        outputs.append(out if out else "")
    merged = ("\n\n--- [GEO-Chunk Split] ---\n\n").join(outputs).strip()
    return merged, text

# ============ GEO-Score è‡ªåŠ¨è¯„åˆ† ============
def run_score(original_text, optimized_text, model_name):
    if not optimized_text or optimized_text.startswith(("âš ï¸","âŒ")):
        return "âš ï¸ æ— æ³•è¯„åˆ†ï¼Œè¯·å…ˆç”Ÿæˆä¼˜åŒ–ç¨¿ã€‚", {}
    query = original_text[:80] if len(original_text) > 80 else original_text
    scoring_model = "qwen3-max"
    try:
        score = evaluate_geo_score(
            model_name=scoring_model, query=query,
            src_text=original_text, opt_text=optimized_text,
            mode="single_text", samples=2
        )
        try:
            log_run(model=scoring_model, query=query,
                    original_text=original_text, optimized_text=optimized_text,
                    score_dict=score, mode="single_text")
        except Exception:
            pass
        numeric_items = {k:v for k,v in score.items() if isinstance(v,(int,float))}
        lines = [f"**{k}**ï¼š{v:.2f}" for k,v in numeric_items.items()]
        return f"âœ… GEO-Scoreï¼š{score.get('geo_score',0):.1f} / 100\n\n" + " | ".join(lines), score
    except Exception as e:
        return f"âŒ è¯„åˆ†å¤±è´¥ï¼š{e}", {}

# ============ æŠ¥å‘Šå¯¼å‡ºï¼ˆå¸¦è¯„åˆ†ï¼‰ ============
def export_html_with_score(original_text, optimized_text, score, project_name="", client_name=""):
    if not optimized_text or optimized_text.startswith(("âš ï¸","âŒ")):
        return gr.update(value=None, visible=False), "âš ï¸ å½“å‰æ²¡æœ‰å¯å¯¼å‡ºçš„ä¼˜åŒ–ç»“æœã€‚"
    geo_id = str(uuid.uuid4())[:8]
    base_title = "GEO-Max Report"
    pieces = [p for p in [project_name, client_name, base_title] if p]
    title = " Â· ".join(pieces) + f" Â· ID:{geo_id}"
    html_str = render_report_html(title, original_text, original_text, optimized_text, score or {})
    html_str += f"\n<!-- GEO-ID:{geo_id} -->\n"
    safe_proj = (project_name or "").strip().replace(" ","_")
    safe_clt  = (client_name or "").strip().replace(" ","_")
    ts = datetime.now().strftime("%Y%m%d-%H%M%S")
    fname_bits = ["geo_report", ts]
    if safe_proj: fname_bits.append(safe_proj)
    if safe_clt:  fname_bits.append(safe_clt)
    fname_bits.append(geo_id)
    fname = "_".join(fname_bits) + ".html"
    path = os.path.abspath(os.path.join(SAVE_DIR, fname))
    with open(path,"w",encoding="utf-8") as f: f.write(html_str)
    return gr.update(value=path, visible=True), f"âœ… å·²å¯¼å‡ºå¸¦è¯„åˆ†æŠ¥å‘Šï¼š{path}"

# ======================= GEO-CoT å¢é‡åŠŸèƒ½ï¼ˆæ–°å¢ï¼‰ =======================
# â€”â€” ä¸æ”¹åŠ¨ä½ ç°æœ‰å‡½æ•°ï¼Œä»…æ–°å¢ä¸€ç»„ geo_cot_* æ–¹æ³•ä¸ä¸€ä¸ª Tab â€”â€”

# ç»Ÿä¸€è·¯ç”±åˆ°ä½ ç°æœ‰çš„ä¸‰å®¶æ¨¡å‹
def geo_cot_model_call(prompt: str, provider: str) -> str:
    if provider == "DeepSeek": return call_deepseek(prompt)
    if provider == "é€šä¹‰åƒé—®":   return call_tongyi(prompt)
    if provider == "æ–‡å¿ƒä¸€è¨€":   return call_wenxin(prompt)
    return "âš ï¸ æœªé€‰æ‹©æ¨¡å‹ã€‚"

COT_TRIGGER = "Let's think step by step."

GEO_COT_TASK_REQUIRE = """ä½ æ˜¯GEO-Maxçš„å†…å®¹ç­–ç•¥ä¸æ¨ç†ä¸“å®¶ã€‚è¯·ä¸¥æ ¼æŒ‰ä¸‹åˆ—ç»“æ„è¾“å‡ºJSONï¼š
{
  "logic_chain": ["...èŠ‚ç‚¹1","...èŠ‚ç‚¹2","..."],
  "evidence_chain": [
     {"node":"èŠ‚ç‚¹1","evidence":{"data":"", "industry":"", "media":"", "extra":""},"gaps":""}
  ],
  "titles_by_node": [
     {"node":"èŠ‚ç‚¹1","titles":["",""]}
  ]
}
è¦æ±‚ï¼š
- å»ºè®®3â€“6ä¸ªé€»è¾‘èŠ‚ç‚¹ï¼ˆå¯åœ¨3â€“8å†…æµ®åŠ¨ï¼‰ï¼›é€èŠ‚ç‚¹å¯¹é½è¯æ®ï¼›
- ä¸å¾—è™šæ„å…·ä½“æ•°æ®ï¼›è‹¥è¯æ®ä¸è¶³è¯·åœ¨ gaps ä¸­æ ‡æ³¨é‡‡é›†å»ºè®®ï¼›
- åªè¾“å‡ºJSONï¼Œå‹¿åŠ å¤šä½™è¯´æ˜ã€‚
"""

def geo_cot_assemble_prompt(q: str, brand_ctx: str, exposure_goals: List[str]) -> str:
    return textwrap.dedent(f"""
    [è§¦å‘è¯­]
    {COT_TRIGGER}

    {GEO_COT_TASK_REQUIRE}

    [ç›®æ ‡é—®é¢˜]
    {q}

    [ç”²æ–¹èµ„æ–™]
    {brand_ctx[:1200]}

    [æœŸæœ›éœ²å‡º]
    { "ã€".join([g for g in exposure_goals if g]) }
    """).strip()


def _find_balanced_json_blocks(text: str, max_blocks: int = 6) -> list[str]:
    if not text:
        return []
    blocks, stack = [], []
    start = None
    for i, ch in enumerate(text):
        if ch == '{':
            stack.append(i)
            if start is None:
                start = i
        elif ch == '}':
            if stack:
                stack.pop()
                if not stack and start is not None:
                    blocks.append(text[start:i+1])
                    start = None
        if len(blocks) >= max_blocks:
            break
    return blocks

def geo_cot_extract_json(text: str):
    """ä»æ–‡æœ¬ä¸­æ‰¾å‡ºå€™é€‰ {..}ï¼Œé€ä¸ª json.loadsï¼›ä¼˜å…ˆè¿”å›å­—æ®µé½å…¨çš„é‚£å—"""
    cand_blocks = _find_balanced_json_blocks(text or "")
    best, best_score = None, -1
    for blk in cand_blocks:
        try:
            obj = json.loads(blk)
        except Exception:
            continue
        if not isinstance(obj, dict):
            continue
        score = 0
        for k in ("logic_chain", "evidence_chain", "titles_by_node"):
            if k in obj:
                score += 1
        if score > best_score:
            best, best_score = obj, score
            if score == 3:
                break
    return best

# --------- è½»é‡é‡è¯•ï¼šç©º/æŠ¥é”™æ—¶å†è¯•ä¸€æ¬¡ ---------
def _retry_call(fn, times=2):
    last = None
    for _ in range(max(1, times)):
        try:
            out = fn()
            last = out
            # æˆåŠŸåˆ¤å®šï¼šæœ‰å†…å®¹ä¸”éå‘Šè­¦/æŠ¥é”™æç¤º
            if out and not str(out).startswith(("âš ï¸", "âŒ")):
                return out
        except Exception as e:
            # è®°å½•å¹¶ç»§ç»­é‡è¯•
            print("[MODEL CALL RETRY]", repr(e))
            last = f"âŒ è°ƒç”¨å¼‚å¸¸ï¼š{e}"
            continue
    return last



# ä¸€æ®µå¼ï¼šä¸€æ¬¡æç¤ºï¼Œç›´æ¥äº§å‡º JSONï¼ˆæ–°ç‰ˆå…œåº•å¯¹é½ã€Šé€šç”¨é€»è¾‘ç­–ç•¥ã€‹ï¼‰
def geo_cot_run_once(q, brand_ctx, exposure_text, provider, progress: gr.Progress = None):
    """
    ä¸€æ®µå¼ï¼šä¸€æ¬¡è°ƒç”¨ç›´æ¥äº§å‡ºâ€œé€»è¾‘é“¾ + è¯æ®é“¾ + å¯¹åº”æ ‡é¢˜â€
    è¿”å› (data: dict, None)ï¼Œå¹¶ä¿è¯ç»“æ„å…œåº•ä¸ raw_textï¼ˆæ–°å…œåº•å­—æ®µï¼‰
    """
    goals = [x.strip() for x in (exposure_text or "").split(",") if x.strip()]
    goals_text = ", ".join(goals)

    safe_progress(progress, 0.25, desc="ç»„è£…æç¤ºè¯")
    _, _, p_single = get_cot_prompts(
        user_q=q,
        brand_brief=brand_ctx or "",
        must_expose=goals_text,
        expo_hint="",
        mode="single"
    )

    safe_progress(progress, 0.50, desc="è¯·æ±‚æ¨¡å‹")
    out = _retry_call(lambda: geo_cot_model_call(p_single, provider), times=2)

    print("==== RAW OUTPUT (SINGLE) ====")
    try:
        print(out if len(str(out)) < 4000 else str(out)[:4000] + " â€¦[trunc]")
    except Exception:
        print("<non-printable>")
    print("=============================")

    data = geo_cot_extract_json(out or "")
    if not isinstance(data, dict):
        data = {}

    # === å…œåº•ç»“æ„ï¼šé€»è¾‘é“¾ ===
    if not isinstance(data.get("logic_chain"), list) or not data.get("logic_chain"):
        data["logic_chain"] = [
            "å®šä¹‰å“ç±»æ ‡å‡†ä¸ç”¨æˆ·å…¸å‹è¯‰æ±‚",
            "å°†å“ç‰Œå±æ€§ä¸å“ç±»æ ‡å‡†é€é¡¹ç»‘å®š",
            "æ„å»ºç»Ÿä¸€æ¨èç†ç”±è¯­æ³•ï¼ˆåŠŸèƒ½/æƒ…ç»ª/æ¦œå•ï¼‰",
            "æä¾›ç»“æ„åŒ–å†…å®¹ä¸ Schema ä»¥æå‡å¯å¼•ç”¨æ€§",
            "å»ºç«‹ç›‘æµ‹ä¸è¿­ä»£æœºåˆ¶ï¼ˆGEO-Score å›è·¯ï¼‰"
        ]

    # === å…œåº•ç»“æ„ï¼šè¯æ®é“¾ï¼ˆæŒ‰ä½ çš„â€œè¯æ®é“¾æ•°æ®æ¸…å•â€å­—æ®µï¼‰===
    if not isinstance(data.get("evidence_chain"), list) or not data.get("evidence_chain"):
        node0 = data["logic_chain"][0]
        data["evidence_chain"] = [
            {
                "node": node0,
                "evidence": {
                    "official": "å“ç‰Œå®šä¹‰/å®šä½/æ„¿æ™¯ï¼ˆAbout/Schema åŒæ­¥ï¼‰",
                    "category_tags": "è¡Œä¸šç±»åˆ«/ç»†åˆ†é¢†åŸŸçš„ç»Ÿä¸€æ ‡ç­¾ä¸å‘½å",
                    "products": "ä»£è¡¨äº§å“/ç³»åˆ—ï¼ˆSKU å…ƒæ•°æ®ä¸æ ¸å¿ƒæè¿°ï¼‰",
                    "scenarios": "ç›®æ ‡ç”¨æˆ·/ä½¿ç”¨åœºæ™¯ï¼ˆFAQ/å›¾æ–‡è¯´æ˜ï¼‰",
                    "media_refs": "ç¬¬ä¸‰æ–¹åª’ä½“/æ¦œå•/è¯ä¹¦ï¼ˆç”¨äºå¯ä¿¡å¼•ç”¨ï¼‰",
                    "tech_specs": "å‚æ•°/ææ–™/æ ‡å‡†ï¼ˆç™½çš®ä¹¦/æµ‹è¯„æ‘˜è¦ï¼‰",
                    "third_party": "ç™¾ç§‘/é—®ç­”/ç ”ç©¶æ–‡ç« ç­‰å¤–éƒ¨å¼•ç”¨",
                    "structure": "ç»Ÿä¸€å‘½åå­—æ®µï¼šName/Category/Keywords/Tagline/USPs/HeroProducts/Awards/OfficialLinks/MediaMentions/AudienceFit"
                },
                "gaps": [
                    "è¡¥å…… 1â€“2 ä¸ªå¯é‡åŒ–æŒ‡æ ‡ï¼ˆå¦‚è¿‘90å¤©è¢«å¼•ç”¨ç‡ã€é—®ç­”é‡‡çº³ç‡ï¼‰",
                    "ä¸ºä»£è¡¨äº§å“æ·»åŠ  JSON-LDï¼ˆProduct/FAQ/HowToï¼‰"
                ]
            }
        ]

    # === å…œåº•ç»“æ„ï¼šæ ‡é¢˜ä¸è§¦å‘ï¼ˆä¿æŒæ¯èŠ‚ç‚¹ 2â€“3 æ¡ï¼‰===
    if not isinstance(data.get("titles_by_node"), list) or not data.get("titles_by_node"):
        data["titles_by_node"] = [
            {
                "node": data["logic_chain"][0],
                "titles": [
                    "ä»€ä¹ˆæ˜¯åˆæ ¼çš„ X ç±»å“ç‰Œï¼Ÿ",
                    "ä»å“ç±»æ ‡å‡†åˆ°ç»Ÿä¸€è¯­æ³•ï¼šAI ä¸ºä½•æ¨èä½ "
                ]
            }
        ]

    data["raw_text"] = (out or "")[:2000]
    safe_progress(progress, 0.85, desc="å®Œæˆ")
    return data, None



# ä¸¤æ®µå¼ï¼šå…ˆé•¿æ¨ç†ï¼Œå†æŠ½å– JSONï¼ˆæ›´ç¨³ï¼‰
# GEO_COT_EXTRACT_ONLY = load_md_prompt_file("cot_extract")

def geo_cot_run_two_stage(q, brand_ctx, exposure_text, provider, progress: gr.Progress = None):
    """
    ä¸¤æ®µå¼ï¼ˆæ¨èï¼‰ï¼šStage1 äº§å‡ºâ€œç­–ç•¥æ€§é€»è¾‘é“¾+ç­–ç•¥ä¸Šä¸‹æ–‡â€ï¼ŒStage2 å®Œæˆâ€œè¯æ®é“¾+ä¸¥æ ¼å¯¹åº”æ ‡é¢˜â€ã€‚
    ä»è¿”å› (data: dict, None)ï¼Œå…¶ä¸­ data è‡³å°‘åŒ…å« logic_chain / evidence_chain / titles_by_node / raw_text
    """
    # æœŸæœ›éœ²å‡º -> goals åˆ—è¡¨ä¸å±•ç¤ºæ–‡æœ¬
    goals = [x.strip() for x in (exposure_text or "").split(",") if x.strip()]
    goals_text = ", ".join(goals)

    # ===== Stage 1ï¼šç­–ç•¥æ€§é€»è¾‘é“¾ï¼ˆPLANï¼‰ =====
    safe_progress(progress, 0.20, desc="é˜¶æ®µ1ï¼šç­–ç•¥è§„åˆ’ï¼ˆPLANï¼‰")
    p1, _, _ = get_cot_prompts(
        user_q=q,
        brand_brief=brand_ctx or "",
        must_expose=goals_text,
        expo_hint="",              # éœ€è¦çš„è¯ä½ å¯ä»¥ä»åˆ«å¤„ä¼ å…¥
        mode="two-stage"
    )

    out1 = _retry_call(lambda: geo_cot_model_call(p1, provider), times=2)
    # è®°å½•åŸå§‹æ–‡æœ¬ç‰‡æ®µåˆ° raw_textï¼ˆä¾¿äºè°ƒè¯•ï¼‰
    raw1 = (out1 or "")[:2000]

    s1 = geo_cot_extract_json(out1 or "") or {}
    if not isinstance(s1, dict):
        s1 = {}
    # å…œåº•ï¼šè‹¥æ¨¡å‹æœªè¿”å› logic_chainï¼Œä¹Ÿç»™ä¸ªæœ€å°å¯ç”¨ç­–ç•¥é“¾é¿å…åç»­æŠ¥é”™
    if not isinstance(s1.get("logic_chain"), list) or not s1.get("logic_chain"):
        s1["logic_chain"] = [
            "æ˜ç¡®åŒ—ææ˜Ÿä¸å—ä¼—åˆ†å±‚",
            "æ„å»ºå†…å®¹æ æ†ä¸èµ„äº§å½¢æ€",
            "è§„åˆ’åˆ†å‘ä¸ç»“æ„åŒ–è¦ç´ ",
            "å»ºç«‹å¼•ç”¨ä¸å¤è¿°çš„æç¤ºè¯­è§„èŒƒ",
            "æ­å»ºæµ‹é‡ä¸æ»šåŠ¨è¡¥è¯é—­ç¯"
        ]

    # ===== Stage 2ï¼šè¯æ®é“¾ + å¯¹åº”æ ‡é¢˜ï¼ˆFILLï¼‰ =====
    safe_progress(progress, 0.55, desc="é˜¶æ®µ2ï¼šè¯æ®é“¾ç”Ÿæˆï¼ˆFILLï¼‰")
    # å°† Stage1 çš„ç»“æ„å›çŒåˆ° Stage2 prompt
    _, p2, _ = get_cot_prompts(
        user_q=q,
        brand_brief=brand_ctx or "",
        must_expose=goals_text,
        expo_hint="",
        mode="two-stage",
        stage1_json=s1
    )

    out2 = _retry_call(lambda: geo_cot_model_call(p2, provider), times=2)
    raw2 = (out2 or "")[:2000]

    data = geo_cot_extract_json(out2 or "")

    # ===== å…œåº•ä¸æ¸…æ´— =====
    if not isinstance(data, dict):
        data = {}

    # æŠŠ Stage1 çš„ logic_chain ä½œä¸ºæœ€ç»ˆé“¾æ¡æ¥æºï¼ˆè‹¥ Stage2 æ²¡æœ‰ï¼‰
    if not isinstance(data.get("logic_chain"), list) or not data.get("logic_chain"):
        data["logic_chain"] = s1.get("logic_chain", [])

    if not isinstance(data.get("evidence_chain"), list):
        data["evidence_chain"] = []
    if not isinstance(data.get("titles_by_node"), list):
        data["titles_by_node"] = []

    # è‹¥ä¾æ—§ä¸ºç©ºï¼Œå†™å…¥æœ€å°å¯ç”¨ç»“æ„ï¼Œé˜²æ­¢å‰ç«¯è¶Šç•Œ
    if not data["evidence_chain"]:
        data["evidence_chain"] = [{
            "node": data["logic_chain"][0] if data["logic_chain"] else "æ˜ç¡®åŒ—ææ˜Ÿä¸å—ä¼—åˆ†å±‚",
            "evidence": {
                "data": "ç¤ºä¾‹ï¼šå¹³å°Aè¿‘90å¤©è¢«å¼•ç”¨ç‡12.4%",
                "industry": "è½»å¤åŸæ›´é€‚åˆ how-to + compare çš„ç»„åˆé—®æ³•",
                "media": "ç™¾ç§‘/ç¤¾åŒº/ç¤¾äº¤ä½œä¸ºè¾…åŠ©è¯æ®æ¥æº",
                "extra": "JSON-LD: FAQ + HowToï¼›é¦–æ®µç»Ÿä¸€å£å¾„å¹¶æ˜¾å¼ citation"
            },
            "gaps": "é‡‡é›†å¹³å°è¿‘90å¤©å¼•ç”¨ç‡ä¸é—®ç­”é‡‡çº³ç‡ï¼›è¡¥å……ç¤ºä¾‹é—®æ³•"
        }]

    if not data["titles_by_node"]:
        node0 = data["evidence_chain"][0].get("node", "æ˜ç¡®åŒ—ææ˜Ÿä¸å—ä¼—åˆ†å±‚")
        data["titles_by_node"] = [{
            "node": node0,
            "titles": ["è½»å¤åŸæ€ä¹ˆé€‰ï¼šä»ç›®æ ‡åˆ°åˆ†å‘", "ä»è¢«çœ‹åˆ°åˆ°è¢«å¼•ç”¨ï¼šGEO æ‰§è¡Œé“¾"]
        }]

    # è°ƒè¯•ç”¨åŸæ–‡ç‰‡æ®µ
    data["raw_text"] = (raw1 + "\n\n" + raw2)[:2000]

    safe_progress(progress, 0.90, desc="æ•´ç†è¾“å‡º")
    return data, None


def geo_cot_score(data: Dict[str, Any], exposure_text: str) -> float:
    try:
        goals = [x.strip() for x in (exposure_text or "").split(",") if x.strip()]
        lc = data.get("logic_chain", [])
        tb = data.get("titles_by_node", [])

        # ç»“æ„
        s_struct = 0
        n = len(lc)
        if 3 <= n <= 8:
            s_struct = 10
            if 4 <= n <= 6: s_struct += 10

        # éœ²å‡º
        blob = json.dumps(data, ensure_ascii=False)
        hit = sum(1 for k in goals if k and (k in blob))
        s_expo = 20.0 * (hit / max(1, len(goals))) if goals else 20.0

        # æ ‡é¢˜
        ok_nodes = 0
        for item in tb:
            cnt = len(item.get("titles", []))
            if 2 <= cnt <= 3: ok_nodes += 1
        s_title = 20.0 * (ok_nodes / max(1, len(lc))) if lc else 0.0

        # å…¶å®ƒå ä½
        s_logic = 10.0
        s_exec  = 10.0
        s_align = 15.0

        total = (
            s_struct * 0.2 + s_align * 0.2 + s_expo * 0.2 +
            s_title * 0.2 + s_logic * 0.1 + s_exec * 0.1
        ) * 5
        return round(total, 1)
    except Exception:
        return 0.0


SAVE_DIR = os.path.join(os.path.expanduser("~"), "GEO-Reports")
os.makedirs(SAVE_DIR, exist_ok=True)

with gr.Blocks(title="GEO-Max å¤šæ¨¡å‹æ–‡æœ¬ä¼˜åŒ–å¼•æ“ï¼ˆå«è¯„åˆ†ï¼‰",
               analytics_enabled=False, theme=APP_THEME, css=APP_CSS) as demo:

    with gr.Group(elem_id="wrap"):
        gr.Markdown("### GEO-Max Â· ç”Ÿæˆå¼å¼•æ“ä¼˜åŒ–\næç®€ã€ç¨³å®šï¼šå†…å®¹æ”¹å†™ + è‡ªåŠ¨è¯„åˆ†ã€‚")

        with gr.Tabs(elem_classes=["tabs"]):
            # ---- Tab 1 ----
            with gr.Tab("âš™ï¸ äº§å“æ¨¡å¼ï¼ˆè´¨é‡è¯„åˆ†ï¼‰"):
                with gr.Row():
                    with gr.Column(scale=1, elem_classes=["stack"]):
                        with gr.Group(elem_classes=["tile"]):
                            inp_text = gr.Textbox(label="âœï¸ è¾“å…¥åŸæ–‡", lines=8, show_copy_button=True)
                            model_dd = gr.Dropdown(choices=["é€šä¹‰åƒé—®","DeepSeek","æ–‡å¿ƒä¸€è¨€"],
                                                   value="é€šä¹‰åƒé—®", label="ğŸ§© é€‰æ‹©æ¨¡å‹")
                            use_chunk = gr.Checkbox(value=True, label="è‡ªåŠ¨åˆ†å—ï¼ˆå»ºè®®å¼€å¯ï¼‰")
                            max_chars = gr.Slider(800, 6000, value=2800, step=100, label="æ¯å—æœ€å¤§å­—æ•°")
                            btn_run = gr.Button("ğŸš€ ç”Ÿæˆ GEO-Max ä¼˜åŒ–ç¨¿", variant="primary")
                            btn_clear = gr.Button("ğŸ§¹ æ¸…ç©º")
                            gr.Markdown("<div class='footnote'>æç¤ºï¼šæˆ‘ä»¬ä¸ä¿å­˜ä½ çš„æ–‡æœ¬ï¼›è¯„åˆ†ä»…åœ¨æœ¬åœ°ä¼šè¯å†…è®¡ç®—ã€‚</div>")

                    with gr.Column(scale=1, elem_classes=["stack"]):
                        with gr.Group(elem_classes=["tile"]):
                            out_text = gr.Textbox(label="ğŸ“ˆ GEO-Max ä¼˜åŒ–ç»“æœ", lines=12, show_copy_button=True)
                            btn_score = gr.Button("ğŸ“Š è®¡ç®— GEO-Scoreï¼ˆè‡ªåŠ¨è¯„åˆ†ï¼‰")
                            score_md = gr.Markdown("")
                            with gr.Row():
                                btn_html = gr.Button("å¯¼å‡ºå¸¦è¯„åˆ†æŠ¥å‘Šï¼ˆHTMLï¼‰")
                                file_html = gr.File(label="ä¸‹è½½æŠ¥å‘Š", visible=False)
                            tip = gr.Markdown("")

                # çŠ¶æ€ä¸äº‹ä»¶
                state_original = gr.State(""); state_optimized = gr.State(""); state_score = gr.State({})
                btn_run.click(fn=run_geo, inputs=[inp_text, model_dd, use_chunk, max_chars],
                              outputs=[out_text, state_original], queue=False)
                out_text.change(lambda x:x, inputs=out_text, outputs=state_optimized, queue=False)
                btn_score.click(fn=run_score, inputs=[state_original, state_optimized, model_dd],
                                outputs=[score_md, state_score], queue=False)
                btn_html.click(fn=export_html_with_score,
                               inputs=[state_original, state_optimized, state_score],
                               outputs=[file_html, tip], queue=False)
                btn_clear.click(lambda: ("","","","",None),
                                None, [inp_text, out_text, score_md, tip, file_html], queue=False)

            # ---- Tab 2 ----
            with gr.Tab("ğŸ“˜ è®ºæ–‡æ¨¡å¼ï¼ˆwith citationsï¼‰"):
                with gr.Row():
                    with gr.Column(scale=1, elem_classes=["stack"]):
                        with gr.Group(elem_classes=["tile"]):
                            n_sources = gr.Number(value=3, label="æ¥æºæ€»æ•°ï¼ˆNï¼‰", precision=0)
                            mode_sel = gr.Dropdown(choices=["WordPos","Word","Pos"], value="WordPos", label="æŒ‡æ ‡æ¨¡å¼")
                            answer_once = gr.Textbox(label="å•æ¬¡åˆ†å¸ƒï¼šå¸¦ [1][2]â€¦ çš„ç­”æ¡ˆï¼ˆä»»ä¸€æ®µï¼‰", lines=6, show_copy_button=True)
                            btn_once = gr.Button("ğŸ“Š è®¡ç®—å•æ¬¡åˆ†å¸ƒ", variant="secondary")
                            msg_once = gr.Markdown("")
                            dist_once = gr.JSON(label="åˆ†å¸ƒï¼ˆå’Œ=1ï¼‰")
                    with gr.Column(scale=1, elem_classes=["stack"]):
                        with gr.Group(elem_classes=["tile"]):
                            before_ans = gr.Textbox(label="Beforeï¼šå¸¦å¼•ç”¨çš„ç­”æ¡ˆ", lines=6, show_copy_button=True)
                            after_ans  = gr.Textbox(label="Afterï¼šå¸¦å¼•ç”¨çš„ç­”æ¡ˆ", lines=6, show_copy_button=True)
                            target_idx = gr.Number(value=1, label="ç›®æ ‡æ¥æºç´¢å¼•ï¼ˆ1..Nï¼‰", precision=0)
                            btn_delta = gr.Button("ğŸ“ˆ è®¡ç®— Î” æå‡ï¼ˆAfter - Beforeï¼‰", variant="primary")
                            msg_delta = gr.Markdown("")
                            res_delta = gr.JSON(label="ç»“æœï¼ˆå« dist_before / dist_after / deltaï¼‰")

                btn_once.click(fn=run_impression_single,
                               inputs=[answer_once, n_sources, mode_sel],
                               outputs=[msg_once, dist_once], queue=False)
                btn_delta.click(fn=run_impression_delta,
                                inputs=[before_ans, after_ans, n_sources, target_idx, mode_sel],
                                outputs=[msg_delta, res_delta], queue=False)

            # ---- Tab 3ï¼ˆé‡å†™ï¼šä¸¤æ®µå¼ Â· çº¯ Markdown æ¨¡æ¿å·¥ä½œæµï¼‰----
            with gr.Tab("ğŸ§  GEO-CoTï¼ˆä¸¤æ®µå¼Â·Markdown æ¨¡æ¿ï¼‰"):
                # ========= ä»…ä¾›æœ¬ Tab ä½¿ç”¨çš„è½»é‡å·¥å…·å‡½æ•° =========
                def _load_md_template(name: str) -> str:
                    """
                    ä» ./geo_prompts/ ç›®å½•åŠ è½½ <name>.md
                    ä¸åšä»»ä½•é€‰æ‹©æ€§è¯»å–æˆ–å­—æ®µé™åˆ¶ï¼›åŸæ ·è¿”å›æ¨¡æ¿æ–‡æœ¬ã€‚
                    """
                    base = os.path.join(os.path.dirname(__file__), "geo_prompts")
                    path = os.path.join(base, f"{name}.md")
                    try:
                        with open(path, "r", encoding="utf-8") as f:
                            return f.read()
                    except Exception as e:
                        return f"âš ï¸ æ— æ³•è¯»å–æ¨¡æ¿ï¼š{path}\n\né”™è¯¯ï¼š{e}"

                # ä»…å…è®¸çš„å ä½ç¬¦ï¼ˆå…¶å®ƒèŠ±æ‹¬å·å…¨éƒ¨è½¬ä¹‰ï¼Œé¿å… .format è¯¯ä¼¤ï¼‰
                _ALLOWED_MD_KEYS = {"USER_QUESTION", "BRAND_BRIEF", "MUST_EXPOSE", "EXPO_HINT", "STAGE1_MD"}

                def _fmt_md_template(tpl: str, **vars) -> str:
                    """
                    å®‰å…¨æ ¼å¼åŒ– Markdown æ¨¡æ¿ï¼š
                    - å…ˆæŠŠæ‰€æœ‰ { å’Œ } è½¬ä¹‰æˆ {{ }} / }}}
                    - å†æŠŠâ€œå…è®¸å ä½ç¬¦â€åè½¬ä¹‰ä¸ºå•å¤§æ‹¬å·
                    - æœ€å .format
                    """
                    if not isinstance(tpl, str):
                        tpl = str(tpl or "")
                    # å…¨é‡è½¬ä¹‰
                    t = tpl.replace("{", "{{").replace("}", "}}")
                    # å…è®¸å ä½ç¬¦åè½¬ä¹‰
                    for key in _ALLOWED_MD_KEYS:
                        t = t.replace("{{" + key + "}}", "{" + key + "}")
                    # æ¸²æŸ“
                    return t.format(**vars)

                def _save_md_to_file(md_text: str, filename: str = "geo_output.md"):
                    try:
                        tmpdir = tempfile.gettempdir()
                        path = os.path.join(tmpdir, filename)
                        with open(path, "w", encoding="utf-8") as f:
                            f.write(md_text or "")
                        return path
                    except Exception:
                        return None

                # ================== Stage 1ï¼šæ‰§è¡Œ cot_stage1.mdï¼Œè¾“å‡º Markdown å¯ç¼–è¾‘ ==================
                def run_stage1_markdown(q: str, brand_ctx: str, expo: str, provider: str, progress=gr.Progress()):
                    """
                    - è¯»å– geo_prompts/cot_stage1.md
                    - ç”¨ {USER_QUESTION}/{BRAND_BRIEF}/{MUST_EXPOSE}/{EXPO_HINT} æ¸²æŸ“
                    - æ¨¡å‹ç”Ÿæˆ Markdownï¼Œç›´æ¥è¿”å›åˆ°â€œå¯ç¼–è¾‘å¤§æ–‡æœ¬æ¡†â€
                    """
                    safe_progress(progress, 0.10, desc="åŠ è½½ Stage1 æ¨¡æ¿ï¼ˆMDï¼‰")
                    tpl = _load_md_template("cot_stage1")
                    if tpl.startswith("âš ï¸ æ— æ³•è¯»å–æ¨¡æ¿"):
                        return tpl, "", None, "âš ï¸ æ¨¡æ¿æœªæ‰¾åˆ°ï¼Œå·²åœ¨ç¼–è¾‘æ¡†è¾“å‡ºé”™è¯¯è¯´æ˜ã€‚"

                    safe_progress(progress, 0.25, desc="æ¸²æŸ“ Stage1 æç¤ºè¯ï¼ˆMDï¼‰")
                    prompt = _fmt_md_template(
                        tpl,
                        USER_QUESTION=(q or "").strip(),
                        BRAND_BRIEF=(brand_ctx or "").strip(),
                        MUST_EXPOSE=(expo or "").strip(),
                        EXPO_HINT=""  # é¢„ç•™å ä½ï¼Œå¿…è¦æ—¶å¯åœ¨ UI åŠ ä¸€ä¸ªè¾“å…¥
                    )

                    safe_progress(progress, 0.55, desc="è¯·æ±‚æ¨¡å‹ï¼ˆStage1ï¼‰")
                    out_md = _retry_call(lambda: geo_cot_model_call(prompt, provider), times=2) or ""
                    if not out_md.strip():
                        out_md = "âš ï¸ æ¨¡å‹æœªè¿”å›å†…å®¹ï¼Œè¯·é‡è¯•æˆ–æ£€æŸ¥æ¨¡æ¿ã€‚"

                    # æä¾›ä¸€ä¸ªä¾¿æ·ä¸‹è½½æŒ‰é’®ï¼ˆå¯é€‰ï¼‰
                    dl_path = _save_md_to_file(out_md, filename="geo_stage1_output.md")
                    safe_progress(progress, 0.90, desc="å®Œæˆ")
                    return out_md, prompt[:1200], dl_path, "âœ… Stage1 å®Œæˆï¼šè¯·åœ¨å·¦ä¾§ç¼–è¾‘åï¼Œç‚¹å‡»è¿›å…¥ Stage2 ç”Ÿæˆè¯æ®é“¾ã€‚"

                # ================== Stage 2ï¼šè¯»å–â€œå·²ç¼–è¾‘çš„ Stage1 MDâ€ï¼Œæ‰§è¡Œ cot_stage2.md ==================
                def run_stage2_markdown(q: str, brand_ctx: str, expo: str, provider: str,
                                        stage1_md: str, progress=gr.Progress()):
                    """
                    - è¯»å– geo_prompts/cot_stage2.md
                    - ç”¨ {USER_QUESTION}/{BRAND_BRIEF}/{MUST_EXPOSE}/{EXPO_HINT}/{STAGE1_MD} æ¸²æŸ“ï¼ˆSTAGE1_MD=ç”¨æˆ·ç¼–è¾‘åçš„å®Œæ•´æ–‡æœ¬ï¼‰
                    - æ¨¡å‹ç”Ÿæˆ Markdown â†’ å±•ç¤º + æ”¯æŒä¸‹è½½
                    """
                    safe_progress(progress, 0.10, desc="åŠ è½½ Stage2 æ¨¡æ¿ï¼ˆMDï¼‰")
                    tpl = _load_md_template("cot_stage2")
                    if tpl.startswith("âš ï¸ æ— æ³•è¯»å–æ¨¡æ¿"):
                        return "> æ— æ³•è¯»å– Stage2 æ¨¡æ¿ã€‚", "", None, "âš ï¸ æ¨¡æ¿æœªæ‰¾åˆ°ã€‚"

                    # ç›´æ¥æŠŠæ•´æ®µ Stage1 MD æ³¨å…¥ {STAGE1_MD}ï¼ˆä¸åšä»»ä½•é™åˆ¶/é€‰æ‹©æ€§è¯»å–ï¼‰
                    safe_progress(progress, 0.28, desc="æ¸²æŸ“ Stage2 æç¤ºè¯ï¼ˆMDï¼‰")
                    prompt = _fmt_md_template(
                        tpl,
                        USER_QUESTION=(q or "").strip(),
                        BRAND_BRIEF=(brand_ctx or "").strip(),
                        MUST_EXPOSE=(expo or "").strip(),
                        EXPO_HINT="",
                        STAGE1_MD=stage1_md or ""
                    )

                    safe_progress(progress, 0.60, desc="è¯·æ±‚æ¨¡å‹ï¼ˆStage2ï¼‰")
                    out_md = _retry_call(lambda: geo_cot_model_call(prompt, provider), times=2) or ""
                    if not out_md.strip():
                        out_md = "> âš ï¸ Stage2 æœªäº§å‡ºå†…å®¹ï¼Œè¯·æ£€æŸ¥ Stage1 æ–‡æ¡£æˆ–æ¨¡æ¿è¯­æ³•ã€‚"

                    # å¯¼å‡º MD
                    dl_path = _save_md_to_file(out_md, filename="geo_stage2_output.md")
                    safe_progress(progress, 0.92, desc="å®Œæˆ")
                    return out_md, prompt[:1200], dl_path, "âœ… Stage2 å®Œæˆï¼šå³ä¾§å¯å¤åˆ¶/ä¸‹è½½æœ€ç»ˆ Markdownã€‚"

                # ================== UIï¼šä¸¤åˆ—å¸ƒå±€ï¼ˆå·¦ï¼šè¾“å…¥ä¸ Stage1ï¼›å³ï¼šStage2ï¼‰ ==================
                with gr.Row():
                    with gr.Column(scale=1, elem_classes=["stack"]):
                        with gr.Group(elem_classes=["tile"]):
                            md_q     = gr.Textbox(label="ğŸ¯ ç›®æ ‡é—®é¢˜", placeholder="ä¾‹å¦‚ï¼šæ¨èå‡ å®¶Ã—Ã—å“ç‰Œ", lines=2)
                            md_brand = gr.Textbox(label="ğŸ·ï¸ ç”²æ–¹èµ„æ–™ï¼ˆæ–‡å­—ï¼‰", lines=6)
                            md_expo  = gr.Textbox(label="ğŸ”— æœŸæœ›éœ²å‡ºï¼ˆé€—å·åˆ†éš”ï¼‰", placeholder="å“ç‰Œå, å®˜ç½‘é“¾æ¥, æŒ‡å®šè¯ç»„", lines=2)
                            md_model = gr.Dropdown(choices=["DeepSeek","é€šä¹‰åƒé—®","æ–‡å¿ƒä¸€è¨€"],
                                                value="DeepSeek", label="ğŸ§© æ¨¡å‹")

                        with gr.Group(elem_classes=["tile"]):
                            gr.Markdown("#### Stage 1ï¼šæ‰§è¡Œ `cot_stage1.md` â†’ ç”Ÿæˆ Markdownï¼ˆå¯ç¼–è¾‘ï¼‰")
                            btn_s1 = gr.Button("ğŸš€ è¿è¡Œ Stage 1ï¼ˆMarkdownï¼‰", variant="primary")
                            s1_md_editable = gr.Textbox(label="ğŸ“ Stage1 äº§å‡ºï¼ˆå¯ç¼–è¾‘ Markdownï¼‰",
                                                        lines=18, show_copy_button=True)
                            s1_prompt_dbg  = gr.Textbox(label="è°ƒè¯•ï¼šStage1 æœ€ç»ˆæç¤ºè¯ç‰‡æ®µï¼ˆåªè¯»ï¼‰",
                                                        lines=5, interactive=False)
                            s1_download    = gr.DownloadButton(label="ä¸‹è½½ Stage1 .md", value=None)
                            s1_tip         = gr.Markdown("")

                            btn_confirm_s2 = gr.Button("âœ… ä½¿ç”¨ä¸Šæ–¹ Markdown è¿›å…¥ Stage 2", variant="secondary")

                    with gr.Column(scale=1, elem_classes=["stack"]):
                        with gr.Group(elem_classes=["tile"]):
                            gr.Markdown("#### Stage 2ï¼šæ‰§è¡Œ `cot_stage2.md`ï¼ˆæ³¨å…¥ä½ ç¼–è¾‘åçš„ Stage1 æ–‡æ¡£ï¼‰")
                            s2_md_view   = gr.Markdown(value="> è¿è¡Œ Stage 2 åï¼Œè¿™é‡Œæ˜¾ç¤ºæœ€ç»ˆ Markdown")
                            s2_prompt_dbg= gr.Textbox(label="è°ƒè¯•ï¼šStage2 æœ€ç»ˆæç¤ºè¯ç‰‡æ®µï¼ˆåªè¯»ï¼‰",
                                                    lines=5, interactive=False)
                            s2_download  = gr.DownloadButton(label="ä¸‹è½½ Stage2 .md", value=None)
                            s2_tip       = gr.Markdown("")

                # ================== äº‹ä»¶ç»‘å®š ==================
                btn_s1.click(
                    run_stage1_markdown,
                    inputs=[md_q, md_brand, md_expo, md_model],
                    outputs=[s1_md_editable, s1_prompt_dbg, s1_download, s1_tip],
                    show_progress=True
                )

                btn_confirm_s2.click(
                    run_stage2_markdown,
                    inputs=[md_q, md_brand, md_expo, md_model, s1_md_editable],
                    outputs=[s2_md_view, s2_prompt_dbg, s2_download, s2_tip],
                    show_progress=True
                )





if __name__ == "__main__":
    try:
        demo.launch(server_name="127.0.0.1", server_port=7862, share=False, show_api=False,
                    allowed_paths=[SAVE_DIR])
    except Exception as e:
        print("âš ï¸ æœ¬æœºç›´è¿å¤±è´¥ï¼Œè‡ªåŠ¨å¯ç”¨åˆ†äº«é“¾æ¥ã€‚åŸå› ï¼š", e)
        demo.launch(server_name="127.0.0.1", server_port=7862, share=True, show_api=False,
                    allowed_paths=[SAVE_DIR])
